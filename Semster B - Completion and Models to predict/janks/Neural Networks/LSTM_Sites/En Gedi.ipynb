{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Prepared by Vytautas Bielinskas. 2020. <br>\n",
    "Download data from: https://finance.yahoo.com/quote/GOOG/history/\n",
    "</p>\n",
    "\n",
    "<h2>PART 1. Data Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #0. Fire the system</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go # for visualization\n",
    "import os\n",
    "import sys \n",
    "sys.path.append(\"../../Function/\")\n",
    "import function \n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #1. Read data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Training Set\n",
    "df = pd.read_excel('../../../site_info_ver_4_lite.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "dataset = dataset[dataset.Site_Name==dataset.Site_Name.unique()[2]]\n",
    "site_name = dataset.Site_Name.unique()[0]\n",
    "print(site_name)\n",
    "# dataset_train.set_index(\"Date\",inplace=True)\n",
    "dataset[['so2','nox','pm10','pm2.5', 'is_Site_exceeded_nox', 'is_Site_exceeded_so2', 'is_Site_exceeded_pm10', 'is_Site_exceeded_pm2.5']].isna().sum()\n",
    "dataset.drop(['so2','nox','pm10','pm2.5','is_Site_exceeded_nox', 'is_Site_exceeded_so2', 'is_Site_exceeded_pm10', 'is_Site_exceeded_pm2.5','Day_before_Total','Tourists_Count', 'Total'],axis=1,inplace=True)\n",
    "dataset = function.remove_unique_one(dataset)\n",
    "dataset = function.remove_high_corr(dataset, 'Israelis_Count')\n",
    "dataset = function.split_date(dataset)\n",
    "print('dataset shape before remove outliers',dataset.shape)\n",
    "dataset = function.remove_outliers(dataset, 'Israelis_Count')\n",
    "tmp = dataset.Israelis_Count\n",
    "dataset.drop('Israelis_Count',axis=1,inplace=True)\n",
    "print('dataset shape after remove outliers',dataset.shape)\n",
    "dataset['Israelis_Count']=tmp\n",
    "# dataset.set_index('Date',inplace=True)\n",
    "print()\n",
    "print(dataset.columns)\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_predict = 1   # Number of days we want top predict into the future\n",
    "n_past = 2   # Number of past days we want to use to predict the future\n",
    "cols = dataset.drop(\"Date\",axis=1).columns\n",
    "target='Israelis_Count'\n",
    "target_title='Israelis'\n",
    "def X_y_data(dataset):\n",
    "    # Select features (columns) to be involved intro training and predictions\n",
    "\n",
    "    # Extract dates (will be used in visualization)\n",
    "    datelist = list(dataset['Date'])\n",
    "    \n",
    "    # datelist = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist]\n",
    "    dataset=dataset.set_index('Date')\n",
    "    print('set shape == {}'.format(dataset.shape))\n",
    "    print('All timestamps == {}'.format(len(datelist)))\n",
    "    print('Featured selected: {}'.format(cols))\n",
    "    dataset = dataset[cols].astype(str)\n",
    "    for i in cols:\n",
    "        for j in dataset.index:\n",
    "            dataset[i][j] = dataset[i][j].replace(',', '')\n",
    "\n",
    "    dataset = dataset.astype(float)\n",
    "\n",
    "    # Using multiple features (predictors)\n",
    "    training_set = dataset.values\n",
    "\n",
    "    print('Shape of set == {}.'.format(training_set.shape))\n",
    "    print(training_set)\n",
    "\n",
    "    #first get log from visitors : \n",
    "    \n",
    "    a = training_set[:,:-1]\n",
    "    b = np.log(training_set[:,-1]+0.001)\n",
    "    print('a',a.shape,a)\n",
    "    print('b',b.shape,b)\n",
    "    # np.reshape(b, b.shape[0],)\n",
    "    training_set = np.insert(a, len(cols)-1,values=b, axis=1) # Insert values before column 3\n",
    "\n",
    "    print('training_set\\n',training_set)\n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "    sc_predict = StandardScaler()\n",
    "    sc_predict.fit_transform(training_set[:, -1:])\n",
    "    # Creating a data structure with 90 timestamps and 1 output\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_past, len(training_set_scaled) - n_predict +1):\n",
    "        X_train.append(training_set_scaled[i - n_past:i,:])\n",
    "        y_train.append(training_set_scaled[i + n_predict - 1:i + n_predict,-1])\n",
    "    print()\n",
    "\n",
    "    return dataset,datelist,np.array(X_train), np.array(y_train),sc_predict\n",
    "size = round(len(dataset)*0.85)\n",
    "\n",
    "dataset_train = dataset[:size]\n",
    "dataset_test = dataset[size:]\n",
    "print('Training print')\n",
    "print()\n",
    "dataset_train,datelist_train,X_train, y_train,sc_predict_train = X_y_data(dataset_train)\n",
    "\n",
    "print('Testing print')\n",
    "print()\n",
    "dataset_test,datelist_test,X_test, y_test,sc_predict_test = X_y_data(dataset_test)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "print('X_test shape == {}.'.format(X_test.shape))\n",
    "print('y_test shape == {}.'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 2. Create a model. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #3. Building the LSTM based Neural Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and packages from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "# from keras.optimizers import ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend\n",
    " \n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(n_past, dataset_train.shape[1])))\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "\n",
    "#Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# # Adding 2nd LSTM layer\n",
    "# model.add(LSTM(units=64, return_sequences=True))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.15))\n",
    "# # Adding 2nd LSTM layer\n",
    "# model.add(LSTM(units=64, return_sequences=True))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.15))\n",
    "# model.add(LSTM(units=32, return_sequences=False))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.15))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = 'adam', loss=[rmse],metrics='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #4. Start training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10,verbose=1, patience=20)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,verbose=1, patience=20)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss',verbose=1 ,save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=1000, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 3. Make future predictions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate list of sequence of days for predictions\n",
    "# datelist_future = pd.date_range(datelist_train[-1], periods=n_predict, freq='1d').tolist()\n",
    "# datelist_future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_test:\n",
    "    datelist_future_.append(this_timestamp.date())\n",
    "# datelist_future_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #5. Make predictions for future dates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "model.load_weights('weights.h5')\n",
    "predictions_future = model.predict(X_test)\n",
    "len(predictions_future)\n",
    "# predictions_train = model.predict(X_train[n_past:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
    "\n",
    "\n",
    "y_pred_future = np.exp(sc_predict_test.inverse_transform(predictions_future))\n",
    "print(predictions_future.shape)\n",
    "\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=[target]).set_index(pd.Series(datelist_test[n_past:]))\n",
    "\n",
    "\n",
    "PREDICTIONS_FUTURE.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #6. Visualize the Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set plot size \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "\n",
    "res = pd.merge(PREDICTIONS_FUTURE[target], dataset_test[target],left_index=True,right_index=True)\n",
    "res.columns=['Predictions','Actual']\n",
    "res['residuals'] = res['Predictions'] - res['Actual']\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse=sqrt(mean_squared_error(res['Predictions'],res['Actual']))\n",
    "print('rmse',rmse)\n",
    "print('std test',np.std(res['Actual']))\n",
    "\n",
    "rmse_str = str(round(rmse,2))\n",
    "\n",
    "\n",
    "model_site_str='model_Gedi'\n",
    "# Check whether the specified path exists or not\n",
    "path = 'LSTM_models/'+model_site_str+'/mod1_rmse'+rmse_str\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "model.save(path)\n",
    "\n",
    "function.plot_line(\n",
    "  prediction=res.Predictions,\n",
    "  actual=res.Actual,\n",
    "  title=target_title+' Predcitions and Acutal Visitors at '+site_name,\n",
    "  path_save='LSTM_models/'+model_site_str+'/',\n",
    "  file_name='mod1_rmse'+ rmse_str+ '.png',\n",
    "  fig_size_tuple=(20,5)\n",
    "\n",
    "  )\n",
    "\n",
    "function.plot_residuals(\n",
    "  prediction=res.Predictions,\n",
    "  actual=res.Actual,\n",
    "  title='Residuals '+site_name,\n",
    "  path_save='LSTM_models/'+model_site_str+'/',\n",
    "  file_name='mod1_rmse'+ rmse_str+ 'residuals.png',\n",
    "  fig_size_tuple=(20,5)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "# from lime.lime_text import RecurrentTabularExplainer\n",
    "class_names=['Israelis_Count']\n",
    "\n",
    "categorical_features = np.argwhere(\n",
    "    np.array([len(set(dataset_train.values[:,x]))\n",
    "    for x in range(dataset_train.values.shape[1])]) <= 10).flatten()\n",
    "print(dataset_train.columns.shape)\n",
    "print(categorical_features.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "explainer = lime.lime_tabular.RecurrentTabularExplainer(X_train, \n",
    "                                                   feature_names=dataset_train.columns,\n",
    "                                                   class_names=['price'], \n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   verbose=True, mode='regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(X_test[0],model.predict,num_features=10)\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the meaning of that?**\n",
    "\n",
    "\n",
    "* left - we can see the 92th test valueâ€™s prediction\n",
    "* middle - important (significant) features: directions and ranges for the prediction\n",
    "* right - summary of results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddf3f600d40d2341c955235ce25b28b4350cbf03e579f97bb09eddb1e42d4194"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
