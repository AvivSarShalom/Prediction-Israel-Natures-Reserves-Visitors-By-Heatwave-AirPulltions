{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Prepared by Vytautas Bielinskas. 2020. <br>\n",
    "Download data from: https://finance.yahoo.com/quote/GOOG/history/\n",
    "</p>\n",
    "\n",
    "<h2>PART 1. Data Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #0. Fire the system</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go # for visualization\n",
    "import os\n",
    "import sys \n",
    "sys.path.append(\"../Function/\")\n",
    "from function import remove_high_corr,remove_outliers,plot_line,plot_residuals\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #1. Read data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Training Set\n",
    "df = pd.read_excel('../../site_info_ver_4_lite.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "dataset = df[df.Site_Name==df.Site_Name.unique()[1]]\n",
    "site_name=df.Site_Name.unique()[1]\n",
    "# dataset_train.set_index(\"Date\",inplace=True)\n",
    "dataset.head(1)\n",
    "dataset[['so2','nox','pm10','pm2.5']].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_predict = 1   # Number of days we want top predict into the future\n",
    "n_past = 7     # Number of past days we want to use to predict the future\n",
    "# cols = ['Temperature','pm10','Tel_Aviv-Yafo_pm10','Israelis_Count']\n",
    "cols = ['Temperature','Israelis_Count']\n",
    "target='Israelis_Count'\n",
    "target_title='Israelis'\n",
    "def X_y_data(dataset):\n",
    "    # Select features (columns) to be involved intro training and predictions\n",
    "\n",
    "    # Extract dates (will be used in visualization)\n",
    "    datelist = list(dataset['Date'])\n",
    "    \n",
    "    # datelist = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist]\n",
    "    dataset=dataset.set_index('Date')\n",
    "    print('set shape == {}'.format(dataset.shape))\n",
    "    print('All timestamps == {}'.format(len(datelist)))\n",
    "    print('Featured selected: {}'.format(cols))\n",
    "    dataset = dataset[cols].astype(str)\n",
    "    for i in cols:\n",
    "        for j in dataset.index:\n",
    "            dataset[i][j] = dataset[i][j].replace(',', '')\n",
    "\n",
    "    dataset = dataset.astype(float)\n",
    "\n",
    "    # Using multiple features (predictors)\n",
    "    training_set = dataset.values\n",
    "\n",
    "    print('Shape of set == {}.'.format(training_set.shape))\n",
    "    print(training_set)\n",
    "\n",
    "    #first get log from visitors : \n",
    "    \n",
    "    a = training_set[:,:-1]\n",
    "    b = np.log(training_set[:,-1]+0.001)\n",
    "    print('a',a.shape,a)\n",
    "    print('b',b.shape,b)\n",
    "    # np.reshape(b, b.shape[0],)\n",
    "    training_set = np.insert(a, len(cols)-1,values=b, axis=1) # Insert values before column 3\n",
    "\n",
    "    print('training_set\\n',training_set)\n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "    sc_predict = StandardScaler()\n",
    "    sc_predict.fit_transform(training_set[:, -1:])\n",
    "    # Creating a data structure with 90 timestamps and 1 output\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_past, len(training_set_scaled) - n_predict +1):\n",
    "        X_train.append(training_set_scaled[i - n_past:i,:])\n",
    "        y_train.append(training_set_scaled[i + n_predict - 1:i + n_predict,-1])\n",
    "    print()\n",
    "\n",
    "    return dataset,datelist,np.array(X_train), np.array(y_train),sc_predict\n",
    "size = round(len(dataset)*.8)\n",
    "\n",
    "dataset_train = dataset[:size]\n",
    "dataset_test = dataset[size:]\n",
    "print('Training print')\n",
    "print()\n",
    "dataset_train,datelist_train,X_train, y_train,sc_predict_train = X_y_data(dataset_train)\n",
    "\n",
    "print('Testing print')\n",
    "print()\n",
    "dataset_test,datelist_test,X_test, y_test,sc_predict_test = X_y_data(dataset_test)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "print('X_test shape == {}.'.format(X_test.shape))\n",
    "print('y_test shape == {}.'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 2. Create a model. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #3. Building the LSTM based Neural Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and packages from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "# from keras.optimizers import ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend\n",
    " \n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(n_past, dataset_train.shape[1])))\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.35))\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(units=32, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=32, return_sequences=True))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=16, return_sequences=False))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Output layer\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = 'adam', loss=[rmse],metrics='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #4. Start training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=20, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=1000, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 3. Make future predictions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate list of sequence of days for predictions\n",
    "# datelist_future = pd.date_range(datelist_train[-1], periods=n_predict, freq='1d').tolist()\n",
    "# datelist_future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_test:\n",
    "    datelist_future_.append(this_timestamp.date())\n",
    "# datelist_future_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #5. Make predictions for future dates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions_future = model.predict(X_test)\n",
    "len(predictions_future)\n",
    "# predictions_train = model.predict(X_train[n_past:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
    "\n",
    "\n",
    "y_pred_future = np.exp(sc_predict_test.inverse_transform(predictions_future))\n",
    "# y_pred_train = np.exp(sc_predict_train.inverse_transform(predictions_train))\n",
    "print(predictions_future.shape)\n",
    "\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=[target]).set_index(pd.Series(datelist_test[n_past:]))\n",
    "# PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=[target]).set_index(pd.Series(datelist_train[2 * n_past + n_predict -1:]))\n",
    "\n",
    "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
    "# PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
    "\n",
    "PREDICTIONS_FUTURE.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #6. Visualize the Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(PREDICTIONS_FUTURE[target], dataset_test[target],left_index=True,right_index=True)\n",
    "res.columns=['Predictions','Actual']\n",
    "res['residuals'] = res['Predictions'] - res['Actual']\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse=sqrt(mean_squared_error(res['Predictions'],res['Actual']))\n",
    "print('rmse',rmse)\n",
    "print('std test',np.std(res['Actual']))\n",
    "\n",
    "rmse_str = str(round(rmse,2))\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "path = 'LSTM_models/model_Eilat/mod1_rmse'+rmse_str\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "model.save(path)\n",
    "\n",
    "plot_line(\n",
    "  predicion=res.Predictions,\n",
    "  actual=res.Actual,\n",
    "  title=target_title+' Predcitions and Acutal Visitors at '+site_name,\n",
    "  path_save='LSTM_models/model_Eilat/',\n",
    "  file_name='mod1_rmse'+ rmse_str+ '.png',\n",
    "  fig_size_tuple=(25,8)\n",
    "  )\n",
    "\n",
    "plot_residuals(\n",
    "  predicion=res.Predictions,\n",
    "  actual=res.Actual,\n",
    "  title=target_title+' Predcitions and Acutal Visitors at '+site_name,\n",
    "  path_save='LSTM_models/model_Eilat/',\n",
    "  file_name='mod1_rmse'+ rmse_str+ 'residuals.png'\n",
    "  )\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse training set timestamp for better visualization\n",
    "dataset_train = pd.DataFrame(dataset_train, columns=cols)\n",
    "dataset_train.index = datelist_train\n",
    "dataset_train.index = pd.to_datetime(dataset_train.index)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ace568d36d830c5571f2829ec101ed577db7d1f44057a629faa8733711eb527"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
