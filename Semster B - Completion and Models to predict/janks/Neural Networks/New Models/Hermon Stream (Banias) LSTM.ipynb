{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Function/')\n",
    "import function\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../../../site_info_ver_4_lite.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove pollution site Successfully\n",
      "model on site Hermon Stream (Banias)\n",
      "Focus on  Israelis\n",
      "shape of dataset (1858, 30)\n",
      "features : Index(['Date', 'is_weekend', 'operations', 'is_jewish_holiday',\n",
      "       'is_muslims_holiday', 'Temperature', 'Haifa_pm2.5', 'Jerusalem_nox',\n",
      "       'Ashkelon_nox', 'Beer-Sheva_nox', 'Jerusalem_so2', 'Haifa_so2',\n",
      "       'Ashkelon_so2', 'Beer-Sheva_so2', 'Tel_Aviv-Yafo_pm10_exceeded',\n",
      "       'Ashkelon_pm10_exceeded', 'Beer-Sheva_pm10_exceeded',\n",
      "       'Haifa_pm2.5_exceeded', 'Beer-Sheva_pm2.5_exceeded',\n",
      "       'Tel_Aviv-Yafo_nox_exceeded', 'Jerusalem_nox_exceeded',\n",
      "       'Haifa_nox_exceeded', 'Season_autumn', 'Season_spring', 'Season_summer',\n",
      "       'Season_winter', 'Israelis_Count', 'day', 'month', 'year'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>operations</th>\n",
       "      <th>is_jewish_holiday</th>\n",
       "      <th>is_muslims_holiday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Haifa_pm2.5</th>\n",
       "      <th>Jerusalem_nox</th>\n",
       "      <th>Ashkelon_nox</th>\n",
       "      <th>Beer-Sheva_nox</th>\n",
       "      <th>Jerusalem_so2</th>\n",
       "      <th>...</th>\n",
       "      <th>Jerusalem_nox_exceeded</th>\n",
       "      <th>Haifa_nox_exceeded</th>\n",
       "      <th>Season_autumn</th>\n",
       "      <th>Season_spring</th>\n",
       "      <th>Season_summer</th>\n",
       "      <th>Season_winter</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Israelis_Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>26.2</td>\n",
       "      <td>501.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.262694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_weekend  operations  is_jewish_holiday  is_muslims_holiday  \\\n",
       "Date                                                                        \n",
       "2016-01-01           1           0                  0                   0   \n",
       "\n",
       "            Temperature  Haifa_pm2.5  Jerusalem_nox  Ashkelon_nox  \\\n",
       "Date                                                                \n",
       "2016-01-01         11.4         26.2          501.7           3.8   \n",
       "\n",
       "            Beer-Sheva_nox  Jerusalem_so2  ...  Jerusalem_nox_exceeded  \\\n",
       "Date                                       ...                           \n",
       "2016-01-01             9.7            0.1  ...                       1   \n",
       "\n",
       "            Haifa_nox_exceeded  Season_autumn  Season_spring  Season_summer  \\\n",
       "Date                                                                          \n",
       "2016-01-01                   1              0              0              0   \n",
       "\n",
       "            Season_winter  day  month  year  Israelis_Count  \n",
       "Date                                                         \n",
       "2016-01-01              1    1      1  2016        4.262694  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = df.copy()\n",
    "sites = dataset.Site_Name.unique()\n",
    "site_name = sites[5]\n",
    "target = 'Israelis_Count'\n",
    "target_title = 'Israelis'\n",
    "dataset = dataset.loc[dataset.Site_Name==site_name]\n",
    "dataset = dataset.drop(['Total','Tourists_Count','Day_before_Total'],axis=1)\n",
    "dataset = function.remove_pollution_site(dataset)\n",
    "# dataset = dataset.drop(dataset.filter(regex='exceeded').columns, axis=1)\n",
    "# dataset = dataset.drop(dataset.filter(regex='Season').columns, axis=1)\n",
    "\n",
    "print('model on site',site_name)\n",
    "print('Focus on ',target_title)\n",
    "\n",
    "dataset = function.remove_outliers(dataset, target)\n",
    "dataset = function.remove_unique_one(dataset)\n",
    "dataset = function.remove_high_corr(dataset, target,0.4)\n",
    "dataset = function.split_date(dataset)\n",
    "# dataset = function.last_year_entries_info(dataset,target)\n",
    "\n",
    "print('shape of dataset',dataset.shape)\n",
    "print('features :',dataset.columns)\n",
    "dataset.set_index('Date',inplace=True)\n",
    "dataset = function.move_target_to_last(dataset, target)\n",
    "dataset.Israelis_Count = np.log(dataset.Israelis_Count+0.001)\n",
    "\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>operations</th>\n",
       "      <th>is_jewish_holiday</th>\n",
       "      <th>is_muslims_holiday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Haifa_pm2.5</th>\n",
       "      <th>Jerusalem_nox</th>\n",
       "      <th>Ashkelon_nox</th>\n",
       "      <th>Beer-Sheva_nox</th>\n",
       "      <th>Jerusalem_so2</th>\n",
       "      <th>...</th>\n",
       "      <th>Jerusalem_nox_exceeded</th>\n",
       "      <th>Haifa_nox_exceeded</th>\n",
       "      <th>Season_autumn</th>\n",
       "      <th>Season_spring</th>\n",
       "      <th>Season_summer</th>\n",
       "      <th>Season_winter</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Israelis_Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>26.2</td>\n",
       "      <td>501.7</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.262694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>60.8</td>\n",
       "      <td>733.1</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>111.6</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>6.669499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>53.9</td>\n",
       "      <td>481.4</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>109.7</td>\n",
       "      <td>479.7</td>\n",
       "      <td>6.643478</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.257670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>124.1</td>\n",
       "      <td>1045.5</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>109.6</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>199.5</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>2020</td>\n",
       "      <td>7.656811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>37.6</td>\n",
       "      <td>261.3</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>48.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>6.939255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>104.3</td>\n",
       "      <td>697.5</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>73.7</td>\n",
       "      <td>1.30</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>5.634793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>92.7</td>\n",
       "      <td>512.1</td>\n",
       "      <td>6.486957</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.659486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>139.6</td>\n",
       "      <td>1114.7</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>116.7</td>\n",
       "      <td>4.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.003890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1858 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_weekend  operations  is_jewish_holiday  is_muslims_holiday  \\\n",
       "Date                                                                        \n",
       "2016-01-01           1           0                  0                   0   \n",
       "2017-01-01           0           0                  1                   0   \n",
       "2018-01-01           0           0                  0                   0   \n",
       "2019-01-01           0           1                  0                   0   \n",
       "2020-01-01           0           0                  0                   0   \n",
       "...                ...         ...                ...                 ...   \n",
       "2020-10-31           1           0                  0                   0   \n",
       "2016-12-31           1           0                  1                   0   \n",
       "2017-12-31           0           0                  0                   0   \n",
       "2018-12-31           0           1                  0                   0   \n",
       "2019-12-31           0           0                  0                   0   \n",
       "\n",
       "            Temperature  Haifa_pm2.5  Jerusalem_nox  Ashkelon_nox  \\\n",
       "Date                                                                \n",
       "2016-01-01         11.4         26.2          501.7      3.800000   \n",
       "2017-01-01         15.3         60.8          733.1     14.200000   \n",
       "2018-01-01         11.3         53.9          481.4     20.000000   \n",
       "2019-01-01         19.6        109.7          479.7      6.643478   \n",
       "2020-01-01         15.2        124.1         1045.5     21.200000   \n",
       "...                 ...          ...            ...           ...   \n",
       "2020-10-31         30.9         95.0          199.5      8.900000   \n",
       "2016-12-31         11.1         37.6          261.3      7.700000   \n",
       "2017-12-31         16.7        104.3          697.5     18.900000   \n",
       "2018-12-31         13.9         92.7          512.1      6.486957   \n",
       "2019-12-31         12.9        139.6         1114.7     18.000000   \n",
       "\n",
       "            Beer-Sheva_nox  Jerusalem_so2  ...  Jerusalem_nox_exceeded  \\\n",
       "Date                                       ...                           \n",
       "2016-01-01             9.7           0.10  ...                       1   \n",
       "2017-01-01           111.6           0.35  ...                       1   \n",
       "2018-01-01            21.0           1.50  ...                       1   \n",
       "2019-01-01            84.0           0.90  ...                       1   \n",
       "2020-01-01           109.6           4.40  ...                       1   \n",
       "...                    ...            ...  ...                     ...   \n",
       "2020-10-31            32.0           1.50  ...                       1   \n",
       "2016-12-31            48.3           0.60  ...                       1   \n",
       "2017-12-31            73.7           1.30  ...                       1   \n",
       "2018-12-31            61.9           0.90  ...                       1   \n",
       "2019-12-31           116.7           4.80  ...                       1   \n",
       "\n",
       "            Haifa_nox_exceeded  Season_autumn  Season_spring  Season_summer  \\\n",
       "Date                                                                          \n",
       "2016-01-01                   1              0              0              0   \n",
       "2017-01-01                   1              0              0              0   \n",
       "2018-01-01                   1              0              0              0   \n",
       "2019-01-01                   1              0              0              0   \n",
       "2020-01-01                   1              0              0              0   \n",
       "...                        ...            ...            ...            ...   \n",
       "2020-10-31                   1              1              0              0   \n",
       "2016-12-31                   1              0              0              0   \n",
       "2017-12-31                   1              0              0              0   \n",
       "2018-12-31                   1              0              0              0   \n",
       "2019-12-31                   1              0              0              0   \n",
       "\n",
       "            Season_winter  day  month  year  Israelis_Count  \n",
       "Date                                                         \n",
       "2016-01-01              1    1      1  2016        4.262694  \n",
       "2017-01-01              1    1      1  2017        6.669499  \n",
       "2018-01-01              1    1      1  2018        4.787500  \n",
       "2019-01-01              1    1      1  2019        6.257670  \n",
       "2020-01-01              1    1      1  2020        6.033089  \n",
       "...                   ...  ...    ...   ...             ...  \n",
       "2020-10-31              0   31     10  2020        7.656811  \n",
       "2016-12-31              1   31     12  2016        6.939255  \n",
       "2017-12-31              1   31     12  2017        5.634793  \n",
       "2018-12-31              1   31     12  2018        5.659486  \n",
       "2019-12-31              1   31     12  2019        6.003890  \n",
       "\n",
       "[1858 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.sort_values(['day','month'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateValues = dataset[['day','month','year']].loc[dataset.year>2016].values\n",
    "# size = int(np.round(len(dateValues)*0.8))\n",
    "# dateValues_train = dateValues[:size]\n",
    "# dateValues_test = dateValues[size:]\n",
    "\n",
    "# print('train', len(dateValues_train))\n",
    "# print('test', len(dateValues_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n",
      "354\n",
      "358\n",
      "339\n",
      "277\n",
      "171\n",
      "train 1687\n",
      "test 787\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop(dataset.filter(regex='exceeded').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='so2').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='nox').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='pm10').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='pm2.5').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='Season').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='Season').columns, axis=1)\n",
    "dataValues_train = dataset.loc[dataset.year<=2020].values\n",
    "dataValues_test = dataset.loc[dataset.year>=2019].values\n",
    "\n",
    "print(len(dataset.loc[dataset.year==2016]))\n",
    "print(len(dataset.loc[dataset.year==2017]))\n",
    "print(len(dataset.loc[dataset.year==2018]))\n",
    "print(len(dataset.loc[dataset.year==2019]))\n",
    "print(len(dataset.loc[dataset.year==2020]))\n",
    "print(len(dataset.loc[dataset.year==2021]))\n",
    "\n",
    "print('train', len(dataValues_train))\n",
    "print('test', len(dataValues_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_scaler = StandardScaler()\n",
    "test_scaler = StandardScaler()\n",
    "y_test_scaler = StandardScaler()\n",
    "\n",
    "dataValues_train_scaled = train_scaler.fit_transform(dataValues_train)\n",
    "dataValues_test_scaled = test_scaler.fit_transform(dataValues_test)\n",
    "\n",
    "y_test_scaled = y_test_scaler.fit_transform(dataValues_test[:,-1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape == (973, 2, 9).\n",
      "y_train shape == (973,).\n",
      "X_test shape == (155, 2, 9).\n",
      "y_test shape == (155,).\n"
     ]
    }
   ],
   "source": [
    "n_past = 2\n",
    "n_predict = 1\n",
    "\n",
    "X_train =[]\n",
    "y_train =[]\n",
    "for i in range(len(dataValues_train_scaled)):\n",
    "    if dataValues_train[i][-2] ==2016 or dataValues_train[i][-2] == 2017 or dataValues_train[i][-3] != dataValues_train[i-1][-3] or dataValues_train[i][-4] != dataValues_train[i-1][-4] :\n",
    "        continue\n",
    "    X_train.append(dataValues_train_scaled[i-n_past:i])\n",
    "    y_train.append(dataValues_train_scaled[i:i+1,-1 :])\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "    \n",
    "for i in range(len(dataValues_test_scaled)):\n",
    "    if dataValues_test[i][-2] == 2019 or dataValues_test[i][-2] == 2020 or dataValues_test[i][-3] != dataValues_test[i-1][-3] or dataValues_test[i][-4] != dataValues_test[i-1][-4]:\n",
    "        continue\n",
    "    X_test.append(dataValues_test_scaled[i-n_past:i])\n",
    "    y_test.append(dataValues_test[i:i+1,-1:])\n",
    "  \n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "print('X_test shape == {}.'.format(X_test.shape))\n",
    "print('y_test shape == {}.'.format(y_test.shape))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "# Import Libraries and packages from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# from keras.optimizers import ADAM\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=8, return_sequences=True, input_shape=(n_past, dataValues_train_scaled.shape[1])))\n",
    "# Adding Dropout\n",
    "# model.add(Dropout(0.35))\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=8, return_sequences=True))\n",
    "\n",
    "#Adding Dropout\n",
    "# model.add(Dropout(0.))\n",
    "# model.add(LSTM(units=68, return_sequences=True))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.35))\n",
    "\n",
    "# model.add(LSTM(units=68, return_sequences=True))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.35))\n",
    "\n",
    "# model.add(LSTM(units=68, return_sequences=True))\n",
    "\n",
    "# #Adding Dropout\n",
    "# model.add(Dropout(0.35))\n",
    "\n",
    "# model.add(LSTM(units=68, return_sequences=True))\n",
    "\n",
    "#Adding Dropout\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(LSTM(units=8, return_sequences=False))\n",
    "\n",
    "#Adding Dropout\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = 'adam', loss=[rmse],metrics='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "580/583 [============================>.] - ETA: 0s - loss: 0.8004 - mean_squared_error: 1.0833\n",
      "Epoch 00001: val_loss improved from inf to 0.69345, saving model to weights.h5\n",
      "583/583 [==============================] - 12s 9ms/step - loss: 0.8000 - mean_squared_error: 1.0812 - val_loss: 0.6935 - val_mean_squared_error: 0.7232 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "582/583 [============================>.] - ETA: 0s - loss: 0.7482 - mean_squared_error: 0.9662\n",
      "Epoch 00002: val_loss improved from 0.69345 to 0.68466, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7478 - mean_squared_error: 0.9650 - val_loss: 0.6847 - val_mean_squared_error: 0.7085 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "583/583 [==============================] - ETA: 0s - loss: 0.7406 - mean_squared_error: 0.9358\n",
      "Epoch 00003: val_loss improved from 0.68466 to 0.64678, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7406 - mean_squared_error: 0.9358 - val_loss: 0.6468 - val_mean_squared_error: 0.6606 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "581/583 [============================>.] - ETA: 0s - loss: 0.7256 - mean_squared_error: 0.9261\n",
      "Epoch 00004: val_loss did not improve from 0.64678\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.7251 - mean_squared_error: 0.9240 - val_loss: 0.6620 - val_mean_squared_error: 0.6765 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "576/583 [============================>.] - ETA: 0s - loss: 0.7052 - mean_squared_error: 0.8536\n",
      "Epoch 00005: val_loss improved from 0.64678 to 0.64448, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7064 - mean_squared_error: 0.8554 - val_loss: 0.6445 - val_mean_squared_error: 0.6506 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "582/583 [============================>.] - ETA: 0s - loss: 0.7156 - mean_squared_error: 0.8857\n",
      "Epoch 00006: val_loss improved from 0.64448 to 0.64163, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7146 - mean_squared_error: 0.8842 - val_loss: 0.6416 - val_mean_squared_error: 0.6501 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "576/583 [============================>.] - ETA: 0s - loss: 0.7134 - mean_squared_error: 0.8753\n",
      "Epoch 00007: val_loss improved from 0.64163 to 0.61992, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7098 - mean_squared_error: 0.8680 - val_loss: 0.6199 - val_mean_squared_error: 0.6354 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "582/583 [============================>.] - ETA: 0s - loss: 0.7114 - mean_squared_error: 0.8762\n",
      "Epoch 00008: val_loss did not improve from 0.61992\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.7102 - mean_squared_error: 0.8747 - val_loss: 0.6270 - val_mean_squared_error: 0.6373 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "579/583 [============================>.] - ETA: 0s - loss: 0.6946 - mean_squared_error: 0.8674\n",
      "Epoch 00009: val_loss did not improve from 0.61992\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6929 - mean_squared_error: 0.8638 - val_loss: 0.6265 - val_mean_squared_error: 0.6399 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "582/583 [============================>.] - ETA: 0s - loss: 0.6888 - mean_squared_error: 0.8566\n",
      "Epoch 00010: val_loss improved from 0.61992 to 0.61721, saving model to weights.h5\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6878 - mean_squared_error: 0.8552 - val_loss: 0.6172 - val_mean_squared_error: 0.6318 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "578/583 [============================>.] - ETA: 0s - loss: 0.6859 - mean_squared_error: 0.8302\n",
      "Epoch 00011: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6868 - mean_squared_error: 0.8368 - val_loss: 0.6297 - val_mean_squared_error: 0.6378 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "575/583 [============================>.] - ETA: 0s - loss: 0.6936 - mean_squared_error: 0.8242\n",
      "Epoch 00012: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6917 - mean_squared_error: 0.8221 - val_loss: 0.6327 - val_mean_squared_error: 0.6361 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "579/583 [============================>.] - ETA: 0s - loss: 0.6919 - mean_squared_error: 0.8466\n",
      "Epoch 00013: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6889 - mean_squared_error: 0.8414 - val_loss: 0.6188 - val_mean_squared_error: 0.6304 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "573/583 [============================>.] - ETA: 0s - loss: 0.6864 - mean_squared_error: 0.8223\n",
      "Epoch 00014: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6831 - mean_squared_error: 0.8135 - val_loss: 0.6272 - val_mean_squared_error: 0.6404 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "577/583 [============================>.] - ETA: 0s - loss: 0.6836 - mean_squared_error: 0.8042\n",
      "Epoch 00015: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6856 - mean_squared_error: 0.8121 - val_loss: 0.6360 - val_mean_squared_error: 0.6392 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "579/583 [============================>.] - ETA: 0s - loss: 0.6680 - mean_squared_error: 0.7828\n",
      "Epoch 00016: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6708 - mean_squared_error: 0.7872 - val_loss: 0.6284 - val_mean_squared_error: 0.6366 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "575/583 [============================>.] - ETA: 0s - loss: 0.6810 - mean_squared_error: 0.8187\n",
      "Epoch 00017: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6802 - mean_squared_error: 0.8178 - val_loss: 0.6337 - val_mean_squared_error: 0.6377 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "580/583 [============================>.] - ETA: 0s - loss: 0.6742 - mean_squared_error: 0.7967\n",
      "Epoch 00018: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6752 - mean_squared_error: 0.7968 - val_loss: 0.6325 - val_mean_squared_error: 0.6438 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "576/583 [============================>.] - ETA: 0s - loss: 0.6686 - mean_squared_error: 0.7848\n",
      "Epoch 00019: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 7ms/step - loss: 0.6723 - mean_squared_error: 0.7940 - val_loss: 0.6194 - val_mean_squared_error: 0.6290 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "581/583 [============================>.] - ETA: 0s - loss: 0.6763 - mean_squared_error: 0.7935\n",
      "Epoch 00020: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6782 - mean_squared_error: 0.7969 - val_loss: 0.6289 - val_mean_squared_error: 0.6470 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "575/583 [============================>.] - ETA: 0s - loss: 0.6652 - mean_squared_error: 0.7957\n",
      "Epoch 00021: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6660 - mean_squared_error: 0.7937 - val_loss: 0.6279 - val_mean_squared_error: 0.6434 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "575/583 [============================>.] - ETA: 0s - loss: 0.6644 - mean_squared_error: 0.7710\n",
      "Epoch 00022: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6616 - mean_squared_error: 0.7647 - val_loss: 0.6271 - val_mean_squared_error: 0.6481 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "577/583 [============================>.] - ETA: 0s - loss: 0.6711 - mean_squared_error: 0.8011\n",
      "Epoch 00023: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6697 - mean_squared_error: 0.7964 - val_loss: 0.6342 - val_mean_squared_error: 0.6540 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "574/583 [============================>.] - ETA: 0s - loss: 0.6572 - mean_squared_error: 0.7724\n",
      "Epoch 00024: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6584 - mean_squared_error: 0.7725 - val_loss: 0.6456 - val_mean_squared_error: 0.6706 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "583/583 [==============================] - ETA: 0s - loss: 0.6565 - mean_squared_error: 0.7626\n",
      "Epoch 00025: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6565 - mean_squared_error: 0.7626 - val_loss: 0.6367 - val_mean_squared_error: 0.6583 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "576/583 [============================>.] - ETA: 0s - loss: 0.6603 - mean_squared_error: 0.7748\n",
      "Epoch 00026: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6579 - mean_squared_error: 0.7700 - val_loss: 0.6208 - val_mean_squared_error: 0.6301 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "577/583 [============================>.] - ETA: 0s - loss: 0.6544 - mean_squared_error: 0.7673\n",
      "Epoch 00027: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6530 - mean_squared_error: 0.7642 - val_loss: 0.6508 - val_mean_squared_error: 0.6891 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "579/583 [============================>.] - ETA: 0s - loss: 0.6467 - mean_squared_error: 0.7342\n",
      "Epoch 00028: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6470 - mean_squared_error: 0.7336 - val_loss: 0.6525 - val_mean_squared_error: 0.7003 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "581/583 [============================>.] - ETA: 0s - loss: 0.6400 - mean_squared_error: 0.7242\n",
      "Epoch 00029: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6414 - mean_squared_error: 0.7267 - val_loss: 0.6663 - val_mean_squared_error: 0.7089 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "582/583 [============================>.] - ETA: 0s - loss: 0.6489 - mean_squared_error: 0.7448\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.61721\n",
      "583/583 [==============================] - 4s 6ms/step - loss: 0.6486 - mean_squared_error: 0.7440 - val_loss: 0.6448 - val_mean_squared_error: 0.6815 - lr: 0.0010\n",
      "Epoch 00030: early stopping\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=20,verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1,save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=1000, callbacks=[es, rlr, mcp, tb], validation_split=0.4, verbose=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 609.5769899116287\n",
      "rmse 559.0119206322429\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights.h5')\n",
    "prediction = np.exp(y_test_scaler.inverse_transform(model.predict(X_test)))\n",
    "prediction\n",
    "# res2 = pd.DataFrame(\n",
    "#     data={\n",
    "#         'Prediction':prediction.T[0],\n",
    "#         'Actual': y_test.values\n",
    "#     },\n",
    "#     index=y_test.index\n",
    "# )\n",
    "\n",
    "print('rmse',function.get_rmse(prediction, np.exp(y_test)))\n",
    "print('rmse',np.std(np.exp(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(5,)\n",
      "(973, 2, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. StandardScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11432/3138832781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m explainer = LimeTabularExplainerOvr(X_train, \n\u001b[0m\u001b[0;32m     14\u001b[0m                                                    \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                                    \u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lime_stability\\stability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;34m\"\"\"Inherits from the original LimeTabularExplainer class\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         super(LimeTabularExplainerOvr, self).__init__(training_data, mode, training_labels, feature_names,\n\u001b[0m\u001b[0;32m    152\u001b[0m                                                       \u001b[0mcategorical_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_width\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                                                       \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lime\\lime_tabular.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# Though set has no role to play if training data stats are provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \"\"\"\n\u001b[0;32m    765\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    767\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    714\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[0;32m    717\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. StandardScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "# from lime_stability.stability import LimeTabularExplainerOvr\n",
    "\n",
    "\n",
    "# class_names=['Israelis_Count']\n",
    "\n",
    "# categorical_features = np.argwhere(\n",
    "#     np.array([len(set(dataValues_train_scaled[:,x]))\n",
    "#     for x in range(dataValues_train_scaled.shape[1])]) <= 10).flatten()\n",
    "# print(dataset.columns.shape)\n",
    "# print(categorical_features.shape)\n",
    "# print(X_train.shape)\n",
    "\n",
    "# explainer = LimeTabularExplainerOvr(X_train, \n",
    "#                                                    feature_names=dataset.columns,\n",
    "#                                                    class_names=class_names, \n",
    "#                                                    categorical_features=categorical_features, \n",
    "#                                                    verbose=True, mode='regression')\n",
    "\n",
    "# exp = explainer.explain_instance(X_test[0],model.predict,num_features=10)\n",
    "# exp.show_in_notebook(show_table=True)\n",
    "# exp.as_pyplot_figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_str = str(np.round(function.get_rmse(res2.Prediction, res2.Actual),2))\n",
    "import os \n",
    "# Check whether the specified path exists or not\n",
    "path = site_name+'/'+rmse_str\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "model.save(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddf3f600d40d2341c955235ce25b28b4350cbf03e579f97bb09eddb1e42d4194"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
