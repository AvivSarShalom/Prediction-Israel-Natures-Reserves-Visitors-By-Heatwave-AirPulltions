{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Function/')\n",
    "import function\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../../../site_info_ver_4_lite.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove pollution site Successfully\n",
      "model on site Hermon Stream (Banias)\n",
      "Focus on  Israelis\n",
      "Add Last year visitors Successfully\n",
      "shape of dataset (1858, 21)\n",
      "features : Index(['Date', 'is_weekend', 'operations', 'is_jewish_holiday',\n",
      "       'is_muslims_holiday', 'Temperature', 'is_HeatWave', 'Beer-Sheva_pm10',\n",
      "       'Haifa_pm2.5', 'Beer-Sheva_pm2.5', 'Jerusalem_nox', 'Ashkelon_nox',\n",
      "       'Beer-Sheva_nox', 'Jerusalem_so2', 'Ashkelon_so2', 'Beer-Sheva_so2',\n",
      "       'day', 'month', 'year', 'Last_year_visitors', 'Israelis_Count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>operations</th>\n",
       "      <th>is_jewish_holiday</th>\n",
       "      <th>is_muslims_holiday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>is_HeatWave</th>\n",
       "      <th>Beer-Sheva_pm10</th>\n",
       "      <th>Haifa_pm2.5</th>\n",
       "      <th>Beer-Sheva_pm2.5</th>\n",
       "      <th>...</th>\n",
       "      <th>Ashkelon_nox</th>\n",
       "      <th>Beer-Sheva_nox</th>\n",
       "      <th>Jerusalem_so2</th>\n",
       "      <th>Ashkelon_so2</th>\n",
       "      <th>Beer-Sheva_so2</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Last_year_visitors</th>\n",
       "      <th>Israelis_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>26.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>383.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  is_weekend  operations  is_jewish_holiday  is_muslims_holiday  \\\n",
       "0 2016-01-01           1           0                  0                   0   \n",
       "\n",
       "   Temperature  is_HeatWave  Beer-Sheva_pm10  Haifa_pm2.5  Beer-Sheva_pm2.5  \\\n",
       "0         11.4            0              4.9         26.2               2.5   \n",
       "\n",
       "   ...  Ashkelon_nox  Beer-Sheva_nox  Jerusalem_so2  Ashkelon_so2  \\\n",
       "0  ...           3.8             9.7            0.1           1.2   \n",
       "\n",
       "   Beer-Sheva_so2  day  month  year  Last_year_visitors  Israelis_Count  \n",
       "0             0.9    1      1  2016               383.0              71  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = df.copy()\n",
    "sites = dataset.Site_Name.unique()\n",
    "site_name = sites[5]\n",
    "target = 'Israelis_Count'\n",
    "target_title = 'Israelis'\n",
    "dataset = dataset.loc[dataset.Site_Name==site_name]\n",
    "dataset = dataset.drop(['Total','Tourists_Count'],axis=1)\n",
    "dataset = function.remove_pollution_site(dataset)\n",
    "dataset = dataset.drop(dataset.filter(regex='exceeded').columns, axis=1)\n",
    "dataset = dataset.drop(dataset.filter(regex='Season').columns, axis=1)\n",
    "\n",
    "print('model on site',site_name)\n",
    "print('Focus on ',target_title)\n",
    "\n",
    "dataset = function.remove_outliers(dataset, target)\n",
    "dataset = function.remove_unique_one(dataset)\n",
    "dataset = function.remove_high_corr(dataset, target,0.4)\n",
    "dataset = function.split_date(dataset)\n",
    "dataset = function.last_year_entries_info(dataset,target)\n",
    "\n",
    "print('shape of dataset',dataset.shape)\n",
    "print('features :',dataset.columns)\n",
    "\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.drop(target,axis=1)\n",
    "y = dataset[target]\n",
    "# X.drop(['Date','Tel_Aviv-Yafo_nox_exceeded','Season_spring','Season_autumn','Season_summer','Season_winter','Jerusalem_nox_exceeded'],axis=1,inplace=True)\n",
    "X.drop(['Date'],axis=1,inplace=True)\n",
    "# X = X.drop(X.filter(regex='exceeded').columns, axis=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=312148513)\n",
    "\n",
    "X_train_scaler = MinMaxScaler()\n",
    "X_test_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = X_train_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_test_scaler.fit_transform(X_test)\n",
    "\n",
    "y_train_scaled = np.log(y_train+0.001)\n",
    "y_test_scaled = np.log(y_test+0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 523.4434937182957\n",
      "std 554.252981569447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-0.62, 'Beer-Sheva_pm10'),\n",
       " (-0.577, 'month'),\n",
       " (-0.296, 'Ashkelon_nox'),\n",
       " (-0.275, 'Beer-Sheva_pm2.5'),\n",
       " (-0.211, 'operations'),\n",
       " (-0.112, 'Ashkelon_so2'),\n",
       " (0.111, 'Jerusalem_nox'),\n",
       " (0.116, 'Beer-Sheva_so2'),\n",
       " (0.171, 'Jerusalem_so2'),\n",
       " (0.178, 'is_muslims_holiday'),\n",
       " (0.205, 'is_HeatWave'),\n",
       " (0.22, 'year'),\n",
       " (0.272, 'is_weekend'),\n",
       " (0.288, 'day'),\n",
       " (0.322, 'is_jewish_holiday'),\n",
       " (0.384, 'Haifa_pm2.5'),\n",
       " (0.454, 'Beer-Sheva_nox'),\n",
       " (0.616, 'Last_year_visitors'),\n",
       " (0.859, 'Temperature')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X_train_scaled,y_train_scaled)\n",
    "\n",
    "prediction =np.exp(mlr.predict(X_test_scaled))\n",
    "\n",
    "res = pd.DataFrame(\n",
    "    data={\n",
    "        'Prediction':prediction,\n",
    "        'Actual': y_test.values\n",
    "    },\n",
    "    index=y_test.index\n",
    ")\n",
    "\n",
    "print('rmse',function.get_rmse(res.Prediction, res.Actual))\n",
    "print('std',np.std(res.Actual))\n",
    "\n",
    "coef = sorted( list(zip(np.round(mlr.coef_,3).T,X_train.columns)))\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1161/1188 [============================>.] - ETA: 0s - loss: 2.0251 - mean_squared_error: 6.9205 ETA: 0s - loss: 2.0789 - mean_squared_error: 7.\n",
      "Epoch 00001: val_loss improved from inf to 0.98291, saving model to weights.h5\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 2.0070 - mean_squared_error: 6.8127 - val_loss: 0.9829 - val_mean_squared_error: 1.7683 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "1182/1188 [============================>.] - ETA: 0s - loss: 1.0233 - mean_squared_error: 1.6543\n",
      "Epoch 00002: val_loss improved from 0.98291 to 0.66783, saving model to weights.h5\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 1.0236 - mean_squared_error: 1.6544 - val_loss: 0.6678 - val_mean_squared_error: 1.2896 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "1181/1188 [============================>.] - ETA: 0s - loss: 0.9014 - mean_squared_error: 1.2925\n",
      "Epoch 00003: val_loss did not improve from 0.66783\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.9006 - mean_squared_error: 1.2900 - val_loss: 0.7579 - val_mean_squared_error: 1.3345 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "1172/1188 [============================>.] - ETA: 0s - loss: 0.8140 - mean_squared_error: 1.0159\n",
      "Epoch 00004: val_loss did not improve from 0.66783\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.8127 - mean_squared_error: 1.0116 - val_loss: 0.7075 - val_mean_squared_error: 1.2648 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "1186/1188 [============================>.] - ETA: 0s - loss: 0.7141 - mean_squared_error: 0.8262\n",
      "Epoch 00005: val_loss did not improve from 0.66783\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.7136 - mean_squared_error: 0.8253 - val_loss: 0.7256 - val_mean_squared_error: 1.2864 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "1169/1188 [============================>.] - ETA: 0s - loss: 0.6823 - mean_squared_error: 0.7465\n",
      "Epoch 00006: val_loss did not improve from 0.66783\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.6831 - mean_squared_error: 0.7452 - val_loss: 0.7466 - val_mean_squared_error: 1.3155 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "1163/1188 [============================>.] - ETA: 0s - loss: 0.6781 - mean_squared_error: 0.7382\n",
      "Epoch 00007: val_loss did not improve from 0.66783\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.6778 - mean_squared_error: 0.7380 - val_loss: 0.6770 - val_mean_squared_error: 1.2370 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "1173/1188 [============================>.] - ETA: 0s - loss: 0.6588 - mean_squared_error: 0.7048\n",
      "Epoch 00008: val_loss improved from 0.66783 to 0.61983, saving model to weights.h5\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.6585 - mean_squared_error: 0.7044 - val_loss: 0.6198 - val_mean_squared_error: 1.2173 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "1160/1188 [============================>.] - ETA: 0s - loss: 0.6306 - mean_squared_error: 0.6629\n",
      "Epoch 00009: val_loss improved from 0.61983 to 0.61826, saving model to weights.h5\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.6281 - mean_squared_error: 0.6561 - val_loss: 0.6183 - val_mean_squared_error: 1.2009 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "1169/1188 [============================>.] - ETA: 0s - loss: 0.5995 - mean_squared_error: 0.6060\n",
      "Epoch 00010: val_loss did not improve from 0.61826\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5982 - mean_squared_error: 0.6014 - val_loss: 0.6203 - val_mean_squared_error: 1.1778 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "1167/1188 [============================>.] - ETA: 0s - loss: 0.5814 - mean_squared_error: 0.5638\n",
      "Epoch 00011: val_loss did not improve from 0.61826\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5813 - mean_squared_error: 0.5621 - val_loss: 0.7124 - val_mean_squared_error: 1.2585 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "1167/1188 [============================>.] - ETA: 0s - loss: 0.5728 - mean_squared_error: 0.5470\n",
      "Epoch 00012: val_loss improved from 0.61826 to 0.61485, saving model to weights.h5\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5758 - mean_squared_error: 0.5535 - val_loss: 0.6148 - val_mean_squared_error: 1.1667 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "1187/1188 [============================>.] - ETA: 0s - loss: 0.5629 - mean_squared_error: 0.5388\n",
      "Epoch 00013: val_loss improved from 0.61485 to 0.59274, saving model to weights.h5\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5631 - mean_squared_error: 0.5388 - val_loss: 0.5927 - val_mean_squared_error: 1.1926 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "1182/1188 [============================>.] - ETA: 0s - loss: 0.5628 - mean_squared_error: 0.5406\n",
      "Epoch 00014: val_loss did not improve from 0.59274\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5625 - mean_squared_error: 0.5395 - val_loss: 0.5963 - val_mean_squared_error: 1.2025 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "1163/1188 [============================>.] - ETA: 0s - loss: 0.5489 - mean_squared_error: 0.5106\n",
      "Epoch 00015: val_loss improved from 0.59274 to 0.58676, saving model to weights.h5\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5487 - mean_squared_error: 0.5088 - val_loss: 0.5868 - val_mean_squared_error: 1.1652 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "1167/1188 [============================>.] - ETA: 0s - loss: 0.5496 - mean_squared_error: 0.5051\n",
      "Epoch 00016: val_loss did not improve from 0.58676\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5476 - mean_squared_error: 0.5014 - val_loss: 0.5986 - val_mean_squared_error: 1.1546 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "1159/1188 [============================>.] - ETA: 0s - loss: 0.5441 - mean_squared_error: 0.4959\n",
      "Epoch 00017: val_loss improved from 0.58676 to 0.57842, saving model to weights.h5\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5463 - mean_squared_error: 0.5018 - val_loss: 0.5784 - val_mean_squared_error: 1.1419 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "1175/1188 [============================>.] - ETA: 0s - loss: 0.5347 - mean_squared_error: 0.4761\n",
      "Epoch 00018: val_loss did not improve from 0.57842\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5348 - mean_squared_error: 0.4760 - val_loss: 0.5853 - val_mean_squared_error: 1.1531 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "1188/1188 [==============================] - ETA: 0s - loss: 0.5337 - mean_squared_error: 0.4924\n",
      "Epoch 00019: val_loss did not improve from 0.57842\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5337 - mean_squared_error: 0.4924 - val_loss: 0.5876 - val_mean_squared_error: 1.1429 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "1164/1188 [============================>.] - ETA: 0s - loss: 0.5334 - mean_squared_error: 0.4916\n",
      "Epoch 00020: val_loss did not improve from 0.57842\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5327 - mean_squared_error: 0.4895 - val_loss: 0.5856 - val_mean_squared_error: 1.1750 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "1177/1188 [============================>.] - ETA: 0s - loss: 0.5409 - mean_squared_error: 0.4973\n",
      "Epoch 00021: val_loss did not improve from 0.57842\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5397 - mean_squared_error: 0.4951 - val_loss: 0.5910 - val_mean_squared_error: 1.1324 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "1159/1188 [============================>.] - ETA: 0s - loss: 0.5353 - mean_squared_error: 0.4860\n",
      "Epoch 00022: val_loss did not improve from 0.57842\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5341 - mean_squared_error: 0.4834 - val_loss: 0.6021 - val_mean_squared_error: 1.1484 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "1161/1188 [============================>.] - ETA: 0s - loss: 0.5303 - mean_squared_error: 0.4794\n",
      "Epoch 00023: val_loss improved from 0.57842 to 0.56695, saving model to weights.h5\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5296 - mean_squared_error: 0.4774 - val_loss: 0.5670 - val_mean_squared_error: 1.1378 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "1175/1188 [============================>.] - ETA: 0s - loss: 0.5320 - mean_squared_error: 0.4711\n",
      "Epoch 00024: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5328 - mean_squared_error: 0.4740 - val_loss: 0.5770 - val_mean_squared_error: 1.1259 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "1159/1188 [============================>.] - ETA: 0s - loss: 0.5304 - mean_squared_error: 0.4761\n",
      "Epoch 00025: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5308 - mean_squared_error: 0.4764 - val_loss: 0.5784 - val_mean_squared_error: 1.1075 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "1168/1188 [============================>.] - ETA: 0s - loss: 0.5266 - mean_squared_error: 0.4655\n",
      "Epoch 00026: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5294 - mean_squared_error: 0.4704 - val_loss: 0.5739 - val_mean_squared_error: 1.1364 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "1171/1188 [============================>.] - ETA: 0s - loss: 0.5205 - mean_squared_error: 0.4654\n",
      "Epoch 00027: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5204 - mean_squared_error: 0.4644 - val_loss: 0.5739 - val_mean_squared_error: 1.1230 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "1183/1188 [============================>.] - ETA: 0s - loss: 0.5196 - mean_squared_error: 0.4601\n",
      "Epoch 00028: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5193 - mean_squared_error: 0.4593 - val_loss: 0.6061 - val_mean_squared_error: 1.1320 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "1157/1188 [============================>.] - ETA: 0s - loss: 0.5278 - mean_squared_error: 0.4805\n",
      "Epoch 00029: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5299 - mean_squared_error: 0.4877 - val_loss: 0.5811 - val_mean_squared_error: 1.1130 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "1173/1188 [============================>.] - ETA: 0s - loss: 0.5134 - mean_squared_error: 0.4587\n",
      "Epoch 00030: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5163 - mean_squared_error: 0.4640 - val_loss: 0.5973 - val_mean_squared_error: 1.1203 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.5244 - mean_squared_error: 0.4704\n",
      "Epoch 00031: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5243 - mean_squared_error: 0.4705 - val_loss: 0.5972 - val_mean_squared_error: 1.1423 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "1179/1188 [============================>.] - ETA: 0s - loss: 0.5133 - mean_squared_error: 0.4566\n",
      "Epoch 00032: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5135 - mean_squared_error: 0.4559 - val_loss: 0.5690 - val_mean_squared_error: 1.1288 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "1169/1188 [============================>.] - ETA: 0s - loss: 0.5118 - mean_squared_error: 0.4476\n",
      "Epoch 00033: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5129 - mean_squared_error: 0.4487 - val_loss: 0.5829 - val_mean_squared_error: 1.1367 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "1173/1188 [============================>.] - ETA: 0s - loss: 0.5143 - mean_squared_error: 0.4472\n",
      "Epoch 00034: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5115 - mean_squared_error: 0.4431 - val_loss: 0.5842 - val_mean_squared_error: 1.1960 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "1166/1188 [============================>.] - ETA: 0s - loss: 0.5204 - mean_squared_error: 0.4635\n",
      "Epoch 00035: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5180 - mean_squared_error: 0.4595 - val_loss: 0.5745 - val_mean_squared_error: 1.1524 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "1188/1188 [==============================] - ETA: 0s - loss: 0.5103 - mean_squared_error: 0.4560\n",
      "Epoch 00036: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5103 - mean_squared_error: 0.4560 - val_loss: 0.5743 - val_mean_squared_error: 1.1662 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "1177/1188 [============================>.] - ETA: 0s - loss: 0.5120 - mean_squared_error: 0.4494\n",
      "Epoch 00037: val_loss did not improve from 0.56695\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5112 - mean_squared_error: 0.4481 - val_loss: 0.5992 - val_mean_squared_error: 1.1383 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "1184/1188 [============================>.] - ETA: 0s - loss: 0.5136 - mean_squared_error: 0.4596\n",
      "Epoch 00038: val_loss improved from 0.56695 to 0.56687, saving model to weights.h5\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5134 - mean_squared_error: 0.4589 - val_loss: 0.5669 - val_mean_squared_error: 1.1137 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "1177/1188 [============================>.] - ETA: 0s - loss: 0.5214 - mean_squared_error: 0.4733\n",
      "Epoch 00039: val_loss did not improve from 0.56687\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5214 - mean_squared_error: 0.4730 - val_loss: 0.5743 - val_mean_squared_error: 1.1680 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "1167/1188 [============================>.] - ETA: 0s - loss: 0.5142 - mean_squared_error: 0.4526\n",
      "Epoch 00040: val_loss did not improve from 0.56687\n",
      "1188/1188 [==============================] - 2s 2ms/step - loss: 0.5152 - mean_squared_error: 0.4544 - val_loss: 0.5891 - val_mean_squared_error: 1.1463 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "1170/1188 [============================>.] - ETA: 0s - loss: 0.5121 - mean_squared_error: 0.4500\n",
      "Epoch 00041: val_loss did not improve from 0.56687\n",
      "1188/1188 [==============================] - 3s 2ms/step - loss: 0.5147 - mean_squared_error: 0.4561 - val_loss: 0.5730 - val_mean_squared_error: 1.1661 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      " 903/1188 [=====================>........] - ETA: 0s - loss: 0.5071 - mean_squared_error: 0.4446"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/4054171246.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    647\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m     \"\"\"\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    690\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    691\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asars\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    467\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    470\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0;32m    471\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# create ANN model\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Dense(units=16, input_dim=X_train.shape[1],  activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# # model.add(Dense(units=16, activation='relu'))\n",
    "# # model.add(Dropout(0.25))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    " \n",
    "# Compiling the model\n",
    "model.compile(metrics='mean_squared_error', optimizer='adam', loss = [rmse])\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=70, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=70, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, shuffle=True, epochs=1000, callbacks=[es, rlr, mcp, tb],validation_split=0.2,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 481.5160812900728\n",
      "std 554.252981569447\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights.h5')\n",
    "prediction = np.exp(model.predict(X_test_scaled))\n",
    "\n",
    "res2 = pd.DataFrame(\n",
    "    data={\n",
    "        'Prediction':prediction.T[0],\n",
    "        'Actual': y_test.values\n",
    "    },\n",
    "    index=y_test.index\n",
    ")\n",
    "\n",
    "print('rmse',function.get_rmse(res2.Prediction, res2.Actual))\n",
    "print('std',np.std(res2.Actual))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hermon Stream (Banias)/481.52\\assets\n"
     ]
    }
   ],
   "source": [
    "rmse_str = str(np.round(function.get_rmse(res2.Prediction, res2.Actual),2))\n",
    "import os \n",
    "# Check whether the specified path exists or not\n",
    "path = site_name+'/'+rmse_str\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "model.save(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddf3f600d40d2341c955235ce25b28b4350cbf03e579f97bb09eddb1e42d4194"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
