{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vD2wt-zHJ0yh",
    "outputId": "6abd8589-7222-4f3e-a1cc-676fc26515f4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14708/221984601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"ratings_and_sentiments.csv\",encoding= 'unicode_escape')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "data=data[['review_text','num_rating']]\n",
    "data.head()\n",
    "def decontract(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = str(re.sub(\"\\S*\\d\\S*\", \"\", text).strip()) \n",
    "    text=decontract(text)\n",
    "    # tokenize texts\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    texts_clean = []\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation+'...'):  # remove punctuation\n",
    "            # \n",
    "            stem_word = lemmatizer.lemmatize(word,\"v\")  # Lemmatizing word\n",
    "            texts_clean.append(stem_word)\n",
    "\n",
    "    return \" \".join(texts_clean)\n",
    "data['review_text']=data['review_text'].apply(lambda x: process_text(str(x)))\n",
    "data=data.loc[(data['num_rating']>3) | (data['num_rating']<3)]\n",
    "data['num_rating']=data['num_rating'].apply(lambda x: 1 if (x>3)  else 0)\n",
    "data=data.reset_index()\n",
    "data=data[['review_text','num_rating']]\n",
    "data.head()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review_text'], data['num_rating'], test_size=0.2,random_state=42)\n",
    "\n",
    "top_words = 6000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "max_review_length = 130\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train,y_train, epochs=2, batch_size=64)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_se\n",
    "pip install limequences(list_tokenized_test, maxlen=max_review_length)\n",
    "prediction = model.predict(X_test)\n",
    "y_pred = (prediction > 0.5)\n",
    "print(\"Accuracy of the model : \", accuracy_score(y_pred, y_test))\n",
    "print('F1-score: ', f1_score(y_pred, y_test))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_test,y_pred)(1, activation='sigmoid'))\n",
    "model.compile(l\n",
    "from lime.lime_\n",
    "data_to_test=pd.read_csv(\"ratings_and_sentiments.csv\",encoding= 'unicode_escape')[['review_text','num_rating']]text import LimeTextExplainer\n",
    "class_names=['negative','positive']\n",
    "explainer= LimeTextExplainer(class_names=class_names)\n",
    "def predict_proba(arr):\n",
    "  processed=[]\n",
    "  for i in arr:\n",
    "    processed.append(process_text(i))\n",
    "  list_tokenized_ex = tokenizer.texts_to_sequences(processed)\n",
    "  Ex = pad_sequences(list_tokenized_ex, maxlen=max_review_length)\n",
    "  pred=model.predict(Ex)\n",
    "  returnable=[]\n",
    "  for i in pred:\n",
    "    temp=i[0]\n",
    "    returnable.append(np.array([1-temp,temp])) #I would recommend rounding temp and 1-temp off to 2 places\n",
    "  return np.array(returnable)\n",
    "\n",
    "\n",
    "oss='binary_cro\n",
    "print(\"Actual r\n",
    "print(\"Actual rating\",data_to_test['num_rating'][3])\n",
    "\n",
    "print(\"Actual rating\",data_to_test['num_rating'][7614])\n",
    "explainer.explain_instance(data_to_test['review_text'][7614],predict_proba).show_in_notebook(text=True)\n",
    "explainer.explain_instance(data_to_test['review_text'][3],predict_proba).show_in_notebook(text=True)ating\",data_to_test['num_rating'][5])\n",
    "explainer.explain_instance(data_to_test['review_text'][5],predict_proba).show_in_notebook(text=True)ssentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "data_to_test['review_text'].loc[data_to_test['num_rating']<3]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LIME_MEDIUM.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "4ace568d36d830c5571f2829ec101ed577db7d1f44057a629faa8733711eb527"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
