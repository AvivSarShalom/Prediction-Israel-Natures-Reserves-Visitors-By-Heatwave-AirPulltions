{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  \n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "import sys\n",
    "sys.path.append(\"../../Function/\")\n",
    "import function\n",
    "import warnings\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(\"../../../site_info_ver_3.3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.copy()\n",
    "df.drop( ['nox' , 'pm10', 'pm2.5','so2' ],axis=1,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df = function.split_date(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(axis = 1)\n",
    "dummies = pd.get_dummies(df, 'Site_Name')\n",
    "print(dummies.shape)\n",
    "dummies.set_index('Date',inplace=True)\n",
    "dummies.sort_index(inplace=True)\n",
    "print(dummies.shape)\n",
    "\n",
    "# dummies.drop(['Model_number'] , axis = 1, inplace = True)\n",
    "# dummies = dummies.astype({'Total':np.float64,'Israelis_Count':np.float64, 'Tourists_Count':np.float64})\n",
    "dummies.isna().any().sum()\n",
    "print(dummies.shape)\n",
    "\n",
    "# dummies = function.remove_outliers(dummies, 'Israelis_Count')\n",
    "dummies.dropna(inplace=True)\n",
    "print(dummies.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_res = pd.read_excel(\"../POLI/poly.xlsx\")\n",
    "# writer = pd.ExcelWriter('../../poly.xlsx', engine='xlsxwriter')\n",
    "print(dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_corr = function.remove_high_corr(dummies.drop(['Total' , 'Tourists_Count' ],axis=1), 'Israelis_Count', 0.4)\n",
    "dummies.drop(cols_corr,axis=1,inplace=True)\n",
    "cols_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = dict.fromkeys(dummies.select_dtypes(np.float64).columns, np.float32)\n",
    "d = dict.fromkeys(dummies.select_dtypes(np.int64).columns, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = dummies.astype(d)\n",
    "dummies = dummies.astype(dd)\n",
    "dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = round(len(dummies)*.8)\n",
    "dataset_train = dummies[:size]\n",
    "dataset_test = dummies[size:]\n",
    "\n",
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "X_train = dataset_train.drop(targets,axis=1)\n",
    "y_train = dataset_train['Israelis_Count']\n",
    "\n",
    "X_test = dataset_test.drop(targets,axis=1)\n",
    "y_test = dataset_test['Israelis_Count']\n",
    "\n",
    "print('train x shape:',X_train.shape)\n",
    "print('train y shape:',y_train.shape)\n",
    "print('test x shape:',X_test.shape)\n",
    "print('test y shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import Normalizer \n",
    "train_scaler = MinMaxScaler()\n",
    "test_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = train_scaler.fit_transform(X_train)\n",
    "y_train_scaled = np.log([y_train.values+0.01]).T\n",
    "X_test_scaled = test_scaler.fit_transform(X_test)\n",
    "y_test_scaled = np.log([y_test.values+0.01]).T\n",
    "\n",
    "print('train x shape:',X_train_scaled.shape)\n",
    "print('train y shape:',y_train_scaled.shape)\n",
    "print('test x shape:',X_test_scaled.shape)\n",
    "print('test y shape:',y_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "        \n",
    "#fit the x variable to fit a 2rd degree polynomial value\n",
    "X_poly = poly.fit_transform(X_train_scaled)\n",
    "print(X_poly.shape)\n",
    "poly.fit(X_poly, y_train_scaled)\n",
    "\n",
    "pol_lin_reg = LinearRegression()\n",
    "pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the training data\n",
    "predicion_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "#predicion = y_train_scaler.inverse_transform(predicion_scaled)\n",
    "predicion = np.exp(predicion_scaled)\n",
    "#create a pandas series of the results\n",
    "\n",
    "predicion = round(pd.Series(predicion.reshape(predicion.shape[0]), index=y_test.index, name='predicted_entries_train'),ndigits=2)\n",
    "predicion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(data={\n",
    "    'Predicion':predicion.values,\n",
    "    'Actual':y_test.values}\n",
    "    ,index=y_test.index)\n",
    "res[['Predicion']][res.Predicion>10000] = max(res.Actual)+1000\n",
    "res[res.Predicion>10000]\n",
    "res.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function.plot_line(res.Predicion, res.Actual)\n",
    "function.plot_residuals(res.Predicion, res.Actual,ylim=(-2000,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [dummies.Total , dummies.Israelis_Count , dummies.Tourists_Count]\n",
    "\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    #print(target+\"\\n\")\n",
    "    f = open(\"./\"+target+\"_general.txt\" , \"w\")\n",
    "    for i in range(1,5):\n",
    "        to_drop = [x for x in targets if x != target]\n",
    "        X = dummies.drop(to_drop , axis=1)\n",
    "        corr_df = X.corr()\n",
    "        correlated = corr_df[targets[index]].loc[(abs(corr_df[targets[index]]) >= 0.10)]\n",
    "        \n",
    "        correlated = correlated.drop([targets[index]]).index.tolist()\n",
    "        if len(correlated) == 0:\n",
    "            train_df = None\n",
    "            test_df = None\n",
    "            continue\n",
    "        #targets = [model.Total , model.Israelis_Count , model.Tourists_Count]\n",
    "        #targets_vals = ['Israelis_Count','Tourists_Count','Total']\n",
    "        \n",
    "        #if len(X) == 0: return None , None\n",
    "        X = X[correlated]\n",
    "        if len(X) == 0: \n",
    "            train_df = None\n",
    "            test_df = None\n",
    "            continue\n",
    "        \n",
    "        y = ys[index]\n",
    "        #print(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "        # y_train_scaler = MinMaxScaler()\n",
    "        # y_test_scaler = MinMaxScaler()\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "        # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "        # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        poly = PolynomialFeatures(degree=i)\n",
    "        \n",
    "        #fit the x variable to fit a 2rd degree polynomial value\n",
    "        X_poly = poly.fit_transform(X_train_scaled)\n",
    "        poly.fit(X_poly, y_train_scaled)\n",
    "        pol_lin_reg = LinearRegression()\n",
    "        \n",
    "        pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        #train_df.head()\n",
    "        y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        f.write(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        f.write(\"------ TRAIN DATA ------\\n\")\n",
    "        f.write(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        f.write(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        f.write(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        f.write(\"------ TEST DATA ------\\n\")\n",
    "        f.write(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        f.write(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        f.write(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        f.write(\"--------------------------------\\n\")\n",
    "        \n",
    "        coeff = pol_lin_reg.coef_\n",
    "        #print(coeff)\n",
    "        #intercept = pol_lin_reg.intercept_\n",
    "        equation =\"\"+ str(pol_lin_reg.intercept_)\n",
    "        for idx in range(len(X.columns)):\n",
    "            equation += \" + \"+str(round(coeff[idx],4))+\" * \"+str(X.columns[idx])\n",
    "        new_row = {'Descripton':'general model , corr >= 0.10 ,','Degree':str(i),'Target':target,'Model_type':'POLY',\n",
    "        'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "    'RMSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "    'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "    'TEST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "    \n",
    "        poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "    index += 1\n",
    "    f.close()\n",
    "poly_res.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection by uni test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [dummies.Total , dummies.Israelis_Count , dummies.Tourists_Count]\n",
    "\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    #print(target+\"\\n\")\n",
    "    f = open(\"./\"+target+\"_general_UNI.txt\" , \"w\")\n",
    "    to_drop = [x for x in targets if x != target]\n",
    "    print(to_drop)\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        X = dummies.drop(to_drop , axis=1)\n",
    "        \n",
    "        y = ys[index]\n",
    "        chi_selector = SelectKBest(f_regression, k=10)\n",
    "        chi_selector.fit(X, y)\n",
    "        chi_support = chi_selector.get_support()\n",
    "        chi_feature = X.loc[:,chi_support].columns.to_list()\n",
    "        if target in chi_feature : chi_feature.remove(target)\n",
    "        X = X[chi_feature]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "        # y_train_scaler = MinMaxScaler()\n",
    "        # y_test_scaler = MinMaxScaler()\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "        # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "        # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        poly = PolynomialFeatures(degree=i)\n",
    "        \n",
    "        #fit the x variable to fit a 2rd degree polynomial value\n",
    "        X_poly = poly.fit_transform(X_train_scaled)\n",
    "        poly.fit(X_poly, y_train_scaled)\n",
    "        pol_lin_reg = LinearRegression()\n",
    "        \n",
    "        pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        print(train_df.columns)\n",
    "        y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        f.write(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        f.write(\"------ TRAIN DATA ------\\n\")\n",
    "        f.write(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        f.write(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        f.write(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        f.write(\"------ TEST DATA ------\\n\")\n",
    "        f.write(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        f.write(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        f.write(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        f.write(\"--------------------------------\\n\")\n",
    "        \n",
    "        coeff = pol_lin_reg.coef_\n",
    "        #print(coeff)\n",
    "        intercept = pol_lin_reg.intercept_\n",
    "        equation =\"\"+ str(pol_lin_reg.intercept_)\n",
    "        for idx in range(len(X.columns)):\n",
    "            equation += \" + \"+str(round(coeff[idx],4))+\" * \"+str(X.columns[idx])\n",
    "        new_row = {'Descripton':'general model ,Uni var feature selection, k=10, Degree: '+str(i),'Target':target,'Model_type':'POLY',\n",
    "        'Degree' : str(i),'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "    'RMSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "    'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "    'TEST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "    \n",
    "        poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "    index += 1\n",
    "    f.close()\n",
    "poly_res.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with no pollutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.copy()\n",
    "tmp.columns\n",
    "drops = ['Date', 'Site_Name','Model_number', 'pm10', 'pm2.5',\n",
    "       'nox', 'so2', 'Tel_Aviv-Yafo_pm10',\n",
    "       'Jerusalem_pm10', 'Haifa_pm10', 'Ashkelon_pm10', 'Beer-Sheva_pm10',\n",
    "       'Tel_Aviv-Yafo_pm2.5', 'Jerusalem_pm2.5', 'Haifa_pm2.5',\n",
    "       'Ashkelon_pm2.5', 'Beer-Sheva_pm2.5', 'Tel_Aviv-Yafo_nox',\n",
    "       'Jerusalem_nox', 'Haifa_nox', 'Ashkelon_nox', 'Beer-Sheva_nox',\n",
    "       'Tel_Aviv-Yafo_so2', 'Jerusalem_so2', 'Haifa_so2', 'Ashkelon_so2',\n",
    "       'Beer-Sheva_so2', 'is_Site_exceeded_pm10', 'is_Site_exceeded_pm2.5',\n",
    "       'is_Site_exceeded_nox', 'is_Site_exceeded_so2',\n",
    "       'Tel_Aviv-Yafo_pm10_exceeded', 'Jerusalem_pm10_exceeded',\n",
    "       'Haifa_pm10_exceeded', 'Ashkelon_pm10_exceeded',\n",
    "       'Beer-Sheva_pm10_exceeded', 'Tel_Aviv-Yafo_pm2.5_exceeded',\n",
    "       'Jerusalem_pm2.5_exceeded', 'Haifa_pm2.5_exceeded',\n",
    "       'Ashkelon_pm2.5_exceeded', 'Beer-Sheva_pm2.5_exceeded',\n",
    "       'Tel_Aviv-Yafo_so2_exceeded', 'Jerusalem_so2_exceeded',\n",
    "       'Haifa_so2_exceeded', 'Ashkelon_so2_exceeded',\n",
    "       'Beer-Sheva_so2_exceeded', 'Tel_Aviv-Yafo_nox_exceeded',\n",
    "       'Jerusalem_nox_exceeded', 'Haifa_nox_exceeded', 'Ashkelon_nox_exceeded',\n",
    "       'Beer-Sheva_nox_exceeded']\n",
    "tmp.drop(drops , axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = tmp.corr()\n",
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            if colname not in targets:\n",
    "                correlated_features.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.drop(correlated_features , axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [tmp.Total , tmp.Israelis_Count , tmp.Tourists_Count]\n",
    "\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    #print(target+\"\\n\")\n",
    "    #f = open(\"./\"+target+\"_general_UNI.txt\" , \"w\")\n",
    "    #to_drop = [x for x in targets if x != target]\n",
    "    #print(to_drop)\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        X = tmp.drop(['Total' , 'Israelis_Count', 'Tourists_Count' ] , axis=1)\n",
    "        \n",
    "        y = ys[index]\n",
    "        # chi_selector = SelectKBest(f_regression, k=10)\n",
    "        # chi_selector.fit(X, y)\n",
    "        # chi_support = chi_selector.get_support()\n",
    "        # chi_feature = X.loc[:,chi_support].columns.to_list()\n",
    "        # if target in chi_feature : chi_feature.remove(target)\n",
    "        # X = X[chi_feature]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "        # y_train_scaler = MinMaxScaler()\n",
    "        # y_test_scaler = MinMaxScaler()\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "        # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "        # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        poly = PolynomialFeatures(degree=i)\n",
    "        \n",
    "        #fit the x variable to fit a 2rd degree polynomial value\n",
    "        X_poly = poly.fit_transform(X_train_scaled)\n",
    "        poly.fit(X_poly, y_train_scaled)\n",
    "        pol_lin_reg = LinearRegression()\n",
    "        \n",
    "        pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        print(train_df.columns)\n",
    "        y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        print(\"------ TRAIN DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        print(\"------ TEST DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "        \n",
    "        coeff = pol_lin_reg.coef_\n",
    "        #print(coeff)\n",
    "        #intercept = pol_lin_reg.intercept_\n",
    "        equation =\"\"+ str(pol_lin_reg.intercept_)\n",
    "        for idx in range(len(X.columns)):\n",
    "            equation += \" + \"+str(round(coeff[idx],4))+\" * \"+str(X.columns[idx])\n",
    "    #     new_row = {'Descripton':'general model ,Uni var feature selection, k=10, Degree: '+str(i),'Target':target,'Model_type':'POLY',\n",
    "    #     'Degree' : str(i),'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "    # 'RMSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "    # 'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "    # 'TEST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "    \n",
    "        #poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "    index += 1\n",
    "    #f.close()\n",
    "#poly_res.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try to remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(df, series):\n",
    "  q1 = series.quantile(0.25)\n",
    "  q3 = series.quantile(0.75)\n",
    "\n",
    "  if q1*q3 == 0:\n",
    "    iqr = abs(2*(q1+q3))\n",
    "    toprange = iqr\n",
    "    botrange = -toprange\n",
    "  else:\n",
    "    iqr = q3-q1\n",
    "    toprange = q3 + iqr * 1.5\n",
    "    botrange = q1 - iqr * 1.5\n",
    "\n",
    "  outliers_top=df[series > toprange]\n",
    "  outliers_bot= df[series < botrange]\n",
    "  outliers = pd.concat([outliers_bot, outliers_top], axis=0)\n",
    "\n",
    "  return (botrange, toprange, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botrange, toprange, outliers_total = get_outliers(df, df.Total)\n",
    "print(len(outliers_total))\n",
    "botrange, toprange, outliers_israelis = get_outliers(df, df.Israelis_Count)\n",
    "print(len(outliers_israelis))\n",
    "botrange, toprange, outliers_tourists = get_outliers(df, df.Tourists_Count)\n",
    "print(len(outliers_tourists))\n",
    "index_list_drop  = set(list(outliers_total.index) + list(outliers_israelis.index) + list(outliers_tourists.index))\n",
    "index_list = [i for i in df.index if i not in index_list_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df.loc[index_list]\n",
    "df_no_outliers.reset_index(inplace=True, drop=True)\n",
    "df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.drop(['Date','Model_number','Site_Name'] , axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [df_no_outliers.Total , df_no_outliers.Israelis_Count , df_no_outliers.Tourists_Count]\n",
    "\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    #print(target+\"\\n\")\n",
    "    #f = open(\"./\"+target+\"_general_UNI.txt\" , \"w\")\n",
    "    #to_drop = [x for x in targets if x != target]\n",
    "    #print(to_drop)\n",
    "    for i in range(3,4):\n",
    "        \n",
    "        X = df_no_outliers.drop(['Total' , 'Israelis_Count', 'Tourists_Count' ] , axis=1)\n",
    "        \n",
    "        y = ys[index]\n",
    "        chi_selector = SelectKBest(f_regression, k=10)\n",
    "        chi_selector.fit(X, y)\n",
    "        chi_support = chi_selector.get_support()\n",
    "        chi_feature = X.loc[:,chi_support].columns.to_list()\n",
    "        if target in chi_feature : chi_feature.remove(target)\n",
    "        X = X[chi_feature]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "        # y_train_scaler = MinMaxScaler()\n",
    "        # y_test_scaler = MinMaxScaler()\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "        # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "        # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        poly = PolynomialFeatures(degree=i)\n",
    "        \n",
    "        #fit the x variable to fit a 2rd degree polynomial value\n",
    "        X_poly = poly.fit_transform(X_train_scaled)\n",
    "        poly.fit(X_poly, y_train_scaled)\n",
    "        pol_lin_reg = LinearRegression()\n",
    "        \n",
    "        pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        #print(train_df.columns)\n",
    "        y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        print(\"------ TRAIN DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        print(\"------ TEST DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "        \n",
    "        coeff = pol_lin_reg.coef_\n",
    "        #print(coeff)\n",
    "        #intercept = pol_lin_reg.intercept_\n",
    "        equation =\"\"+ str(pol_lin_reg.intercept_)\n",
    "        for idx in range(len(X.columns)):\n",
    "            equation += \" + \"+str(round(coeff[idx],4))+\" * \"+str(X.columns[idx])\n",
    "        print(\"EQUATION : \" + equation)\n",
    "    #     new_row = {'Descripton':'general model ,Uni var feature selection, k=10, Degree: '+str(i),'Target':target,'Model_type':'POLY',\n",
    "    #     'Degree' : str(i),'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "    # 'RMSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "    # 'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "    # 'TEST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "    \n",
    "        #poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "    index += 1\n",
    "    #f.close()\n",
    "#poly_res.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a3f26f005c805b8796d2eac51f643ea47324abf4ab930a59c0414331385d455"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
