{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  \n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "import utils\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poli_regression(model,degrees,img_path):\n",
    "    # corr_df = model.corr()\n",
    "    # if corr_df.empty:\n",
    "    #     return None,None\n",
    "    # correlated = corr_df.Tourists_Count.loc[(abs(corr_df.Tourists_Count)>=0.15)]\n",
    "    # if len(correlated) == 0:\n",
    "    #     return None,None\n",
    "    # correlated = correlated.drop(['Tourists_Count']).index.tolist()\n",
    "    # if len(correlated) == 0:\n",
    "    #     return None,None\n",
    "\n",
    "    \n",
    "\n",
    "    X = model.drop(['Tourists_Count','Site_Name'],axis = 1)\n",
    "    y = model.Tourists_Count\n",
    "    chi_selector = SelectKBest(f_regression, k=10)\n",
    "    chi_selector.fit(X, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.to_list()\n",
    "\n",
    "    X = X[chi_feature]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "    train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "\n",
    "    x_train_scaler = MinMaxScaler()\n",
    "    x_test_scaler = MinMaxScaler()\n",
    "    y_train_scaler = MinMaxScaler()\n",
    "    y_test_scaler = MinMaxScaler()\n",
    "    #print(X_train)\n",
    "    X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "    y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "    y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degrees)\n",
    "\n",
    "    #fit the x variable to fit a 2rd degree polynomial value\n",
    "    X_poly = poly.fit_transform(X_train_scaled)\n",
    "    poly.fit(X_poly, y_train_scaled)\n",
    "    pol_lin_reg = LinearRegression()\n",
    "    pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "    \n",
    "    #predict the training data\n",
    "    y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "    y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "\n",
    "    #create a pandas series of the results\n",
    "    y_train_pred = round(pd.Series(y_train_pred[:,0], index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "    #Add the results to the DF\n",
    "    train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "    \n",
    "    #train_df.head()\n",
    "    y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "    y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "    #create a pandas series of the results\n",
    "    y_test_pred = round(pd.Series(y_test_pred[:,0], index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "    \n",
    "    #Add the results to the DF\n",
    "    test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "    train_df['residuals'] = train_df.predicted_entries_train - train_df.Tourists_Count\n",
    "    test_df['residuals'] = test_df.predicted_entries_test - test_df.Tourists_Count\n",
    "    fig= go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=train_df.predicted_entries_train,\n",
    "            y=train_df.residuals,\n",
    "            mode='markers',\n",
    "            name='train residuals',\n",
    "            marker_color='blue',\n",
    "            marker_size=1.5,\n",
    "            marker_line_width=0,\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=test_df.predicted_entries_test,\n",
    "        y=test_df.residuals,\n",
    "        mode='markers',\n",
    "        name='test residuals',\n",
    "        marker_color='red',\n",
    "        marker_size=1.5,\n",
    "        marker_line_width=0,\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=test_df.predicted_entries_test,\n",
    "        y=test_df.residuals*0,\n",
    "        mode='lines',\n",
    "        name='zero line',\n",
    "        marker_color='black',\n",
    "        marker_size=1.5,\n",
    "        marker_line_width=0,\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Residuals of Predicted Entries\",\n",
    "        xaxis_title=\"Predicted Entries\",\n",
    "        yaxis_title=\"Residuals\",\n",
    "        font=dict(\n",
    "            size=14,\n",
    "            color=\"RebeccaPurple\"\n",
    "        )\n",
    "    )\n",
    "    fig.write_image( img_path+'tourists_UNI_SELECTION_residuals_results_degree_'+str(degrees)+'.png',width = 1500,height = 600)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../../site_info_ver_3.2.xlsx\")\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df = df.dropna(axis = 0)\n",
    "sites = df.Site_Name.unique()\n",
    "\n",
    "#dummies = pd.get_dummies(df, columns =['Site_Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(tmp, drops):\n",
    "    #to_drop = ['Tourists_Count','Israelis_Count','Total']\n",
    "    cols = tmp.columns\n",
    "    res = [x for x in cols if x in drops]\n",
    "    return tmp.drop(res , axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Site_Name</th>\n",
       "      <th>Israelis_Count</th>\n",
       "      <th>Tourists_Count</th>\n",
       "      <th>Total</th>\n",
       "      <th>region_Central</th>\n",
       "      <th>region_Judea_Samaria</th>\n",
       "      <th>region_North</th>\n",
       "      <th>region_South</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>Tel_Aviv-Yafo_nox_exceeded</th>\n",
       "      <th>Jerusalem_nox_exceeded</th>\n",
       "      <th>Haifa_nox_exceeded</th>\n",
       "      <th>Ashkelon_nox_exceeded</th>\n",
       "      <th>Beer-Sheva_nox_exceeded</th>\n",
       "      <th>Green_border</th>\n",
       "      <th>Season_autumn</th>\n",
       "      <th>Season_spring</th>\n",
       "      <th>Season_summer</th>\n",
       "      <th>Season_winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>88</td>\n",
       "      <td>10</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>108</td>\n",
       "      <td>15</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>2021-08-07</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>321</td>\n",
       "      <td>0</td>\n",
       "      <td>321</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2021-08-11</td>\n",
       "      <td>Apollonia</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Site_Name  Israelis_Count  Tourists_Count  Total  \\\n",
       "0   2016-01-02  Apollonia              88              10     98   \n",
       "1   2016-01-04  Apollonia             108              15    123   \n",
       "2   2016-01-05  Apollonia              48               4     52   \n",
       "3   2016-01-06  Apollonia              75               4     79   \n",
       "4   2016-01-07  Apollonia              56               0     56   \n",
       "..         ...        ...             ...             ...    ...   \n",
       "760 2021-08-03  Apollonia             223               0    223   \n",
       "761 2021-08-06  Apollonia              38               0     38   \n",
       "762 2021-08-07  Apollonia              83               0     83   \n",
       "763 2021-08-09  Apollonia             321               0    321   \n",
       "764 2021-08-11  Apollonia              31               0     31   \n",
       "\n",
       "     region_Central  region_Judea_Samaria  region_North  region_South  \\\n",
       "0                 1                     0             0             0   \n",
       "1                 1                     0             0             0   \n",
       "2                 1                     0             0             0   \n",
       "3                 1                     0             0             0   \n",
       "4                 1                     0             0             0   \n",
       "..              ...                   ...           ...           ...   \n",
       "760               1                     0             0             0   \n",
       "761               1                     0             0             0   \n",
       "762               1                     0             0             0   \n",
       "763               1                     0             0             0   \n",
       "764               1                     0             0             0   \n",
       "\n",
       "     is_weekend  ...  Tel_Aviv-Yafo_nox_exceeded  Jerusalem_nox_exceeded  \\\n",
       "0             1  ...                           1                       1   \n",
       "1             0  ...                           1                       1   \n",
       "2             0  ...                           1                       1   \n",
       "3             0  ...                           1                       1   \n",
       "4             0  ...                           1                       1   \n",
       "..          ...  ...                         ...                     ...   \n",
       "760           0  ...                           1                       1   \n",
       "761           1  ...                           1                       1   \n",
       "762           1  ...                           1                       1   \n",
       "763           0  ...                           1                       1   \n",
       "764           0  ...                           1                       1   \n",
       "\n",
       "     Haifa_nox_exceeded  Ashkelon_nox_exceeded  Beer-Sheva_nox_exceeded  \\\n",
       "0                     1                      0                        1   \n",
       "1                     1                      0                        1   \n",
       "2                     1                      0                        1   \n",
       "3                     1                      0                        1   \n",
       "4                     1                      0                        0   \n",
       "..                  ...                    ...                      ...   \n",
       "760                   1                      0                        1   \n",
       "761                   1                      0                        1   \n",
       "762                   1                      0                        1   \n",
       "763                   1                      0                        0   \n",
       "764                   1                      0                        1   \n",
       "\n",
       "     Green_border  Season_autumn  Season_spring  Season_summer  Season_winter  \n",
       "0               0              0              0              0              1  \n",
       "1               0              0              0              0              1  \n",
       "2               0              0              0              0              1  \n",
       "3               0              0              0              0              1  \n",
       "4               0              0              0              0              1  \n",
       "..            ...            ...            ...            ...            ...  \n",
       "760             0              0              0              1              0  \n",
       "761             0              0              0              1              0  \n",
       "762             0              0              0              1              0  \n",
       "763             0              0              0              1              0  \n",
       "764             0              0              0              1              0  \n",
       "\n",
       "[765 rows x 85 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycaret import *\n",
    "\n",
    "#lets choose a site\n",
    "my_site_df = df.loc[df.Site_Name == 'Apollonia']\n",
    "my_site_df.reset_index(drop = True)\n",
    "my_site_df = drop([''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model : by site characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sites = df.loc[df.is_water == 1]\n",
    "arch_sites = df.loc[df.is_archaeology == 1]\n",
    "water_sites = water_sites.dropna(axis = 0)\n",
    "arch_sites = arch_sites.dropna(axis = 0)\n",
    "#cleaning un relevant columns and stuff..\n",
    "cols = ['Tourists_Count','Israelis_Count','Total','Date','Model_number','Site_Name']\n",
    "\n",
    "lookout_sites = df.loc[df.is_lookout == 1]\n",
    "lookout_sites = lookout_sites.dropna(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_high_corr(df2,target,threshold):\n",
    "\n",
    "  target_col = df2.pop(target)\n",
    "  df2.insert(len(df2.columns), target, target_col)\n",
    "  cor_matrix = df2.corr().abs()\n",
    "  corr_df2 = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "  #מתודה שאומרת בי בקורלורציה עם מי\n",
    "  cols = corr_df2.columns.to_list()\n",
    "  list_corr_not_empty=[]\n",
    "\n",
    "  for i in range(len(cols)-1):\n",
    "      tmp = []\n",
    "      for j in range(len(cols)-1):\n",
    "        if abs(corr_df2.iloc[i,j]) >= threshold and cols[i] is not cols[j] :\n",
    "          tmp.append(cols[j])\n",
    "      if len(tmp)>0:\n",
    "          tmp.append(cols[i])\n",
    "          list_corr_not_empty.append(tmp)\n",
    "  def key(p):\n",
    "   return  corr_df2[target][p]\n",
    "  stay = [max(sub,key=key) for sub in list_corr_not_empty]\n",
    "  drops = [ c for sub in list_corr_not_empty for c in sub if c not in stay ]\n",
    "  return list(set(drops))\n",
    "\n",
    "def remove_outliers(df,target_name):\n",
    "  \n",
    "  plt.cla()\n",
    "  bp = plt.boxplot(df[target_name])\n",
    "  minimums = [round(item.get_ydata()[0], 4) for item in bp['caps']][::2]\n",
    "  maximums = [round(item.get_ydata()[0], 4) for item in bp['caps']][1::2]\n",
    "  return df.drop(df [ (df[target_name]>maximums[0])  | (df[target_name]<minimums[0])].index)\n",
    "\n",
    "drops = remove_high_corr(lookout_sites,'Total',0.4)\n",
    "tmp1  = water_sites.drop(drops,axis = 1)\n",
    "tmp1 = remove_outliers(tmp1,'Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pollutions_cols = ['pm10', 'pm2.5','nox', 'so2','Tel_Aviv-Yafo_pm10',\n",
    "       'Jerusalem_pm10', 'Haifa_pm10', 'Ashkelon_pm10', 'Beer-Sheva_pm10',\n",
    "       'Tel_Aviv-Yafo_pm2.5', 'Jerusalem_pm2.5', 'Haifa_pm2.5',\n",
    "       'Ashkelon_pm2.5', 'Beer-Sheva_pm2.5', 'Tel_Aviv-Yafo_nox',\n",
    "       'Jerusalem_nox', 'Haifa_nox', 'Ashkelon_nox', 'Beer-Sheva_nox',\n",
    "       'Tel_Aviv-Yafo_so2', 'Jerusalem_so2', 'Haifa_so2', 'Ashkelon_so2',\n",
    "       'Beer-Sheva_so2', 'is_Site_exceeded_pm10', 'is_Site_exceeded_pm2.5',\n",
    "       'is_Site_exceeded_nox', 'is_Site_exceeded_so2',\n",
    "       'Tel_Aviv-Yafo_pm10_exceeded', 'Jerusalem_pm10_exceeded',\n",
    "       'Haifa_pm10_exceeded', 'Ashkelon_pm10_exceeded',\n",
    "       'Beer-Sheva_pm10_exceeded', 'Tel_Aviv-Yafo_pm2.5_exceeded',\n",
    "       'Jerusalem_pm2.5_exceeded', 'Haifa_pm2.5_exceeded',\n",
    "       'Ashkelon_pm2.5_exceeded', 'Beer-Sheva_pm2.5_exceeded',\n",
    "       'Tel_Aviv-Yafo_so2_exceeded', 'Jerusalem_so2_exceeded',\n",
    "       'Haifa_so2_exceeded', 'Ashkelon_so2_exceeded',\n",
    "       'Beer-Sheva_so2_exceeded', 'Tel_Aviv-Yafo_nox_exceeded',\n",
    "       'Jerusalem_nox_exceeded', 'Haifa_nox_exceeded', 'Ashkelon_nox_exceeded',\n",
    "       'Beer-Sheva_nox_exceeded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.100000\n",
      "l1_ratio_: 0.990000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use automatically configured elastic net algorithm\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "X = drop(tmp1,['Tourists_Count','Date','Model_number','Site_Name','Israelis_Count']+pollutions_cols ) \n",
    "X = X.loc[X.is_weekend == 1]\n",
    "y = X.Total\n",
    "X = drop(X , 'Total')\n",
    "\n",
    "# chi_selector = SelectKBest(f_regression, k=10)\n",
    "# chi_selector.fit(X, y)\n",
    "# chi_support = chi_selector.get_support()\n",
    "# chi_feature = X.loc[:,chi_support].columns.to_list()\n",
    "# X = X[chi_feature]\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=8, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = np.arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % model.alpha_)\n",
    "print('l1_ratio_: %f' % model.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03109078  0.00918883  0.00568784  0.01786054  0.05058537  0.04861002\n",
      " -0.01168168  0.05481467  0.03918468  0.05339442  0.03922843  0.01826356\n",
      "  0.05134352  0.02357182  0.05459176  0.03384868  0.04186659  0.03516843\n",
      "  0.02313986  0.03243503]\n",
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 274202.24901110155, RMSE: 523.6432459328598, MAE : 378.0196586345382\n",
      "\n",
      "R2 TRAIN -0.2725004975256782\n",
      "\n",
      "TRAIN STD 32.21392965702302\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 266203.23061459174, RMSE: 515.9488643408296, MAE : 140.5869090909091\n",
      "\n",
      "R2 TEST -0.06428482542234626\n",
      "\n",
      "TEST STD 31.66582335098881\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.99)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "x_train_scaler = MinMaxScaler()\n",
    "x_test_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "\n",
    "y_train_scaled = np.log(y_train + 0.01)\n",
    "y_test_scaled = np.log(y_test + 0.01)\n",
    "\n",
    "poly = PolynomialFeatures(degree=5) \n",
    "X_poly = poly.fit_transform(X_train_scaled)\n",
    "poly.fit(X_poly , y_train_scaled)\n",
    "#fit the x variable to fit a 2rd degree polynomial value\n",
    "\n",
    "model.fit(X_poly, y_train_scaled)\n",
    "print(cross_val_score(model , X_train_scaled , y_train_scaled , cv = 20))\n",
    "#predict the training data\n",
    "y_train_pred_scaled = model.predict(poly.fit_transform(X_train_scaled))\n",
    "\n",
    "#y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "y_train_pred = np.exp(y_train_pred_scaled)\n",
    "#create a pandas series of the results\n",
    "y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "\n",
    "#Add the results to the DF\n",
    "train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "#train_df.head()\n",
    "y_test_pred_scaled = model.predict(poly.fit_transform(X_test_scaled))\n",
    "\n",
    "y_test_pred = np.exp(y_test_pred_scaled)\n",
    "#y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "#create a pandas series of the results\n",
    "y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "#Add the results to the DF\n",
    "test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "\n",
    "train_mse = metrics.mean_squared_error(train_df['Total'], train_df.predicted_entries_train)\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(train_df['Total'], train_df.predicted_entries_train))\n",
    "train_mae = metrics.mean_absolute_error(train_df['Total'], train_df.predicted_entries_train)\n",
    "train_std = np.std(train_df.predicted_entries_train)\n",
    "r2_train = r2_score(train_df['Total'], train_df.predicted_entries_train)\n",
    "#print(\"Degrees: \"+str(i)+\",Target: \"\\n\")\n",
    "print(\"------ TRAIN DATA ------\\n\")\n",
    "print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "try:\n",
    "    test_mse = metrics.mean_squared_error(test_df['Total'], test_df.predicted_entries_test)\n",
    "except: pass\n",
    "try:\n",
    "    test_rmse = np.sqrt(metrics.mean_squared_error(test_df['Total'], test_df.predicted_entries_test))\n",
    "except: pass\n",
    "try:\n",
    "    test_mae = metrics.mean_absolute_error(test_df['Israelis_Count'], test_df.predicted_entries_test)\n",
    "except: pass\n",
    "try:\n",
    "    r2_test = r2_score(test_df['Israelis_Count'], test_df.predicted_entries_test)\n",
    "except: pass\n",
    "\n",
    "test_std = np.std(test_df.predicted_entries_test)\n",
    "print(\"------ TEST DATA ------\\n\")\n",
    "print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "\n",
    "\n",
    "for site in list(arch_sites.Site_Name):\n",
    "    index = 0\n",
    "    print(\"------\"+site+\"------\")\n",
    "    for target in targets:\n",
    "        \n",
    "        site_df = arch_sites.loc[(arch_sites['Site_Name'] == site) & (arch_sites['is_weekend'] == 0)]\n",
    "        ys = [site_df.Total,site_df.Israelis_Count,site_df.Tourists_Count]\n",
    "        site_df = drop(site_df,cols)\n",
    "\n",
    "        for i in range(1,4):\n",
    "\n",
    "            X = drop(site_df , 'Site_Name')\n",
    "            y = ys[index]\n",
    "            #print(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "            train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "            test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "            x_train_scaler = MinMaxScaler()\n",
    "            x_test_scaler = MinMaxScaler()\n",
    "            # y_train_scaler = MinMaxScaler()\n",
    "            # y_test_scaler = MinMaxScaler()\n",
    "            X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "            X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "            # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "            # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "            y_train_scaled = np.log(y_train + 0.01)\n",
    "            y_test_scaled = np.log(y_test + 0.01)\n",
    "            poly = PolynomialFeatures(degree=i)\n",
    "            \n",
    "            #fit the x variable to fit a 2rd degree polynomial value\n",
    "            X_poly = poly.fit_transform(X_train_scaled)\n",
    "            poly.fit(X_poly, y_train_scaled)\n",
    "\n",
    "            net = ElasticNet(alpha=0.1, l1_ratio=0.84)\n",
    "\n",
    "            #pol_lin_reg = LinearRegression()\n",
    "            \n",
    "            net.fit(X_poly, y_train_scaled)\n",
    "            scores = cross_val_score(net, X, y, cv=10)\n",
    "\n",
    "            # fit model\n",
    "            model.fit(X, y)\n",
    "\n",
    "            print('alpha : %f' % model.alpha_ +\" site:\"+site+\" degree : \"+str(i))\n",
    "            print('l1_ratio_: %f' % model.l1_ratio_)\n",
    "\n",
    "            #predict the training data\n",
    "            y_train_pred_scaled = net.predict(poly.fit_transform(X_train_scaled))\n",
    "            #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "            y_train_pred = np.exp(y_train_pred_scaled)\n",
    "            #create a pandas series of the results\n",
    "            y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "            \n",
    "            #Add the results to the DF\n",
    "            train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "            #train_df.head()\n",
    "            y_test_pred_scaled = net.predict(poly.fit_transform(X_test_scaled))\n",
    "            y_test_pred = np.exp(y_test_pred_scaled)\n",
    "            #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "            #create a pandas series of the results\n",
    "            y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "            #Add the results to the DF\n",
    "            test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "            if train_df is None and test_df is None : continue\n",
    "\n",
    "            train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "            train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            train_std = np.std(train_df.predicted_entries_train)\n",
    "            r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "            print(\"------ TRAIN DATA ------\\n\")\n",
    "            print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "            print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "            print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "            try:\n",
    "                test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            try:\n",
    "                test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "            except: pass\n",
    "            try:\n",
    "                test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            try:\n",
    "                r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            \n",
    "            test_std = np.std(test_df.predicted_entries_test)\n",
    "            print(\"------ TEST DATA ------\\n\")\n",
    "            print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "            print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "            print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "            print(\"--------------------------------\\n\")\n",
    "            \n",
    "        index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_res = pd.read_excel(\"../POLY/poly.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "\n",
    "\n",
    "for site in sites:\n",
    "    index = 0\n",
    "    for target in targets:\n",
    "        \n",
    "        site_df = df.loc[df.Site_Name == site]\n",
    "        ys = [site_df.Total , site_df.Israelis_Count , site_df.Tourists_Count]\n",
    "\n",
    "        dir_str = \"./\"+site\n",
    "        if not os.path.exists(dir_str):\n",
    "            os.mkdir(dir_str)\n",
    "        f = open(dir_str+\"/\"+target+\"_per_site.txt\" , \"w\")\n",
    "\n",
    "        to_drop = [x for x in targets if x != target]\n",
    "        X = site_df.drop(to_drop , axis=1)\n",
    "        corr_df = X.corr()\n",
    "        correlated = corr_df[targets[index]].loc[(abs(corr_df[targets[index]]) >= 0.10)]\n",
    "        if len(correlated) == 0: continue  \n",
    "        correlated = correlated.drop([targets[index]]).index.tolist()\n",
    "        if len(correlated) == 0:\n",
    "            train_df = None\n",
    "            test_df = None\n",
    "            continue\n",
    "        for i in range(1,5):\n",
    "\n",
    "\n",
    "            #targets = [model.Total , model.Israelis_Count , model.Tourists_Count]\n",
    "            #targets_vals = ['Israelis_Count','Tourists_Count','Total']\n",
    "            \n",
    "            #if len(X) == 0: return None , None\n",
    "            X = X[correlated]\n",
    "            if len(X) == 0: \n",
    "                train_df = None\n",
    "                test_df = None\n",
    "                continue\n",
    "            \n",
    "            y = ys[index]\n",
    "            #print(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "            train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "            test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "            x_train_scaler = MinMaxScaler()\n",
    "            x_test_scaler = MinMaxScaler()\n",
    "            # y_train_scaler = MinMaxScaler()\n",
    "            # y_test_scaler = MinMaxScaler()\n",
    "            X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "            X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "            # y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "            # y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))\n",
    "            y_train_scaled = np.log(y_train + 0.01)\n",
    "            y_test_scaled = np.log(y_test + 0.01)\n",
    "            poly = PolynomialFeatures(degree=i)\n",
    "            \n",
    "            #fit the x variable to fit a 2rd degree polynomial value\n",
    "            X_poly = poly.fit_transform(X_train_scaled)\n",
    "            poly.fit(X_poly, y_train_scaled)\n",
    "            pol_lin_reg = LinearRegression()\n",
    "            \n",
    "            pol_lin_reg.fit(X_poly, y_train_scaled)\n",
    "            \n",
    "            #predict the training data\n",
    "            y_train_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_train_scaled))\n",
    "            #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "            y_train_pred = np.exp(y_train_pred_scaled)\n",
    "            #create a pandas series of the results\n",
    "            y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "            \n",
    "            #Add the results to the DF\n",
    "            train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "            #train_df.head()\n",
    "            y_test_pred_scaled = pol_lin_reg.predict(poly.fit_transform(X_test_scaled))\n",
    "            y_test_pred = np.exp(y_test_pred_scaled)\n",
    "            #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "            #create a pandas series of the results\n",
    "            y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "            #Add the results to the DF\n",
    "            test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "            if train_df is None and test_df is None : continue\n",
    "\n",
    "            train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "            train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            train_std = np.std(train_df.predicted_entries_train)\n",
    "            r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "            f.write(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "            f.write(\"------ TRAIN DATA ------\\n\")\n",
    "            f.write(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "            f.write(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "            f.write(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "            try:\n",
    "                test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            try:\n",
    "                test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "            except: pass\n",
    "            try:\n",
    "                test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            try:\n",
    "                r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "            except: pass\n",
    "            \n",
    "            test_std = np.std(test_df.predicted_entries_test)\n",
    "            f.write(\"------ TEST DATA ------\\n\")\n",
    "            f.write(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "            f.write(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "            f.write(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "            f.write(\"--------------------------------\\n\")\n",
    "            \n",
    "            coeff = pol_lin_reg.coef_\n",
    "            #print(coeff)\n",
    "            #intercept = pol_lin_reg.intercept_\n",
    "            equation =\"\"+ str(pol_lin_reg.intercept_)\n",
    "            for idx in range(len(X.columns)):\n",
    "                equation += \" + \"+str(round(coeff[idx],4))+\" * \"+str(X.columns[idx])\n",
    "            new_row = {'Site_Name': site,'Feature_Selection':'by corr >= 0.1','Degree':str(i),'Target':target,'Model_type':'POLY',\n",
    "            'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "        'RMSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "        'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "        'TEST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "        \n",
    "            poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "            #return train_df,test_df\n",
    "        \n",
    "        f.close()\n",
    "        index += 1\n",
    "poly_res.to_excel(\"../POLY/poly.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a3f26f005c805b8796d2eac51f643ea47324abf4ab930a59c0414331385d455"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
