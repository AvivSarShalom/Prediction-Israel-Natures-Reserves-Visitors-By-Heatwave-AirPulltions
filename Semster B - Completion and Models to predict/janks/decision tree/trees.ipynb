{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  \n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "sys.path.append(\"../Function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../site_info_ver_3.2.xlsx\")\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "# df = df.dropna(axis = 0)\n",
    "# dummies = pd.get_dummies(df, 'Site_Name')\n",
    "# dummies.drop(['Model_number','Date'] , axis = 1, inplace = True)\n",
    "# dummies = dummies.astype({'Total':np.float64,'Israelis_Count':np.float64, 'Tourists_Count':np.float64})\n",
    "# dummies.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[25:]\n",
    "cols_to_drop = ['pm10', 'pm2.5', 'nox', 'so2',\n",
    "        'Tel_Aviv-Yafo_pm10', 'Jerusalem_pm10',\n",
    "       'Haifa_pm10', 'Ashkelon_pm10', 'Beer-Sheva_pm10', 'Tel_Aviv-Yafo_pm2.5',\n",
    "       'Jerusalem_pm2.5', 'Haifa_pm2.5', 'Ashkelon_pm2.5', 'Beer-Sheva_pm2.5',\n",
    "       'Tel_Aviv-Yafo_nox', 'Jerusalem_nox', 'Haifa_nox', 'Ashkelon_nox',\n",
    "       'Beer-Sheva_nox', 'Tel_Aviv-Yafo_so2', 'Jerusalem_so2', 'Haifa_so2',\n",
    "       'Ashkelon_so2', 'Beer-Sheva_so2', 'is_Site_exceeded_pm10',\n",
    "       'is_Site_exceeded_pm2.5', 'is_Site_exceeded_nox',\n",
    "       'is_Site_exceeded_so2', 'Tel_Aviv-Yafo_pm10_exceeded',\n",
    "       'Jerusalem_pm10_exceeded', 'Haifa_pm10_exceeded',\n",
    "       'Ashkelon_pm10_exceeded', 'Beer-Sheva_pm10_exceeded',\n",
    "       'Tel_Aviv-Yafo_pm2.5_exceeded', 'Jerusalem_pm2.5_exceeded',\n",
    "       'Haifa_pm2.5_exceeded', 'Ashkelon_pm2.5_exceeded',\n",
    "       'Beer-Sheva_pm2.5_exceeded', 'Tel_Aviv-Yafo_so2_exceeded',\n",
    "       'Jerusalem_so2_exceeded', 'Haifa_so2_exceeded', 'Ashkelon_so2_exceeded',\n",
    "       'Beer-Sheva_so2_exceeded', 'Tel_Aviv-Yafo_nox_exceeded',\n",
    "       'Jerusalem_nox_exceeded', 'Haifa_nox_exceeded', 'Ashkelon_nox_exceeded',\n",
    "       'Beer-Sheva_nox_exceeded']\n",
    "no_pollution_df = df.drop(columns=cols_to_drop , axis=1)\n",
    "no_pollution_df.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_corr(df2,target,threshold):\n",
    "\n",
    "  target_col = df2.pop(target)\n",
    "  df2.insert(len(df2.columns), target, target_col)\n",
    "  cor_matrix = df2.corr().abs()\n",
    "  corr_df2 = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "  #מתודה שאומרת בי בקורלורציה עם מי\n",
    "  cols = corr_df2.columns.to_list()\n",
    "  list_corr_not_empty=[]\n",
    "\n",
    "  for i in range(len(cols)-1):\n",
    "      tmp = []\n",
    "      for j in range(len(cols)-1):\n",
    "        if abs(corr_df2.iloc[i,j]) >= threshold and cols[i] is not cols[j] :\n",
    "          tmp.append(cols[j])\n",
    "      if len(tmp)>0:\n",
    "          tmp.append(cols[i])\n",
    "          list_corr_not_empty.append(tmp)\n",
    "  def key(p):\n",
    "   return  corr_df2[target][p]\n",
    "  stay = [max(sub,key=key) for sub in list_corr_not_empty]\n",
    "  drops = [ c for sub in list_corr_not_empty for c in sub if c not in stay ]\n",
    "  return list(set(drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,target_name):\n",
    "  \n",
    "  plt.cla()\n",
    "  bp = plt.boxplot(df[target_name])\n",
    "  minimums = [round(item.get_ydata()[0], 4) for item in bp['caps']][::2]\n",
    "  maximums = [round(item.get_ydata()[0], 4) for item in bp['caps']][1::2]\n",
    "  return df.drop(df [ (df[target_name]>maximums[0])  | (df[target_name]<minimums[0])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(df, series):\n",
    "  q1 = series.quantile(0.25)\n",
    "  q3 = series.quantile(0.75)\n",
    "\n",
    "  if q1*q3 == 0:\n",
    "    iqr = abs(2*(q1+q3))\n",
    "    toprange = iqr\n",
    "    botrange = -toprange\n",
    "  else:\n",
    "    iqr = q3-q1\n",
    "    toprange = q3 + iqr * 1.5\n",
    "    botrange = q1 - iqr * 1.5\n",
    "\n",
    "  outliers_top=df[series > toprange]\n",
    "  outliers_bot= df[series < botrange]\n",
    "  outliers = pd.concat([outliers_bot, outliers_top], axis=0)\n",
    "\n",
    "  return (botrange, toprange, outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_corr = df.drop(remove_high_corr(df,'Total',0.4),axis=1)\n",
    "israelis_no_corr = df.drop(remove_high_corr(df ,'Israelis_Count',0.4),axis=1)\n",
    "tourists_no_corr = df.drop(remove_high_corr(df,'Tourists_Count',0.4),axis=1)\n",
    "\n",
    "to_drop = ['Date','Site_Name']\n",
    "total_no_corr.drop(to_drop , axis=1 , inplace=True)\n",
    "israelis_no_corr.drop(to_drop , axis=1 , inplace=True)\n",
    "tourists_no_corr.drop(to_drop , axis=1 , inplace=True)\n",
    "\n",
    "botrange, toprange, outliers_total = get_outliers(df, df.Total)\n",
    "index_total = [i for i in df.index if i not in list(outliers_total.index)]\n",
    "total_no_corr_no_outliers = total_no_corr.loc[index_total]\n",
    "\n",
    "botrange, toprange, outliers_israelis = get_outliers(df, df.Israelis_Count)\n",
    "index_israelis = [i for i in df.index if i not in list(outliers_israelis.index)]\n",
    "israelis_no_corr_no_outliers = israelis_no_corr.loc[index_israelis]\n",
    "\n",
    "botrange, toprange, outliers_tourists = get_outliers(df, df.Tourists_Count)\n",
    "index_tourists = [i for i in df.index if i not in list(outliers_tourists.index)]\n",
    "tourists_no_corr_no_outliers = tourists_no_corr.loc[index_tourists]\n",
    "\n",
    "# total_no_corr_no_outliers = remove_outliers(total_no_corr,'Total')\n",
    "# israelis_no_corr_no_outliers = remove_outliers(israelis_no_corr,'Israelis_Count')\n",
    "# tourists_no_corr_no_outliers = remove_outliers(tourists_no_corr,'Tourists_Count')\n",
    "\n",
    "total_no_corr_no_outliers_week = total_no_corr_no_outliers.loc[total_no_corr_no_outliers.is_weekend == 0]\n",
    "israelis_no_corr_no_outliers_week = israelis_no_corr_no_outliers.loc[israelis_no_corr_no_outliers.is_weekend == 0]\n",
    "tourists_no_corr_no_outliers_week = tourists_no_corr_no_outliers.loc[tourists_no_corr_no_outliers.is_weekend == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df.Site_Name.unique()\n",
    "\n",
    "df.drop(['Date','Site_Name'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 293650.0005973578, RMSE: 541.894824294676, MAE : 87.03722736497548\n",
      "\n",
      "R2 TRAIN 0.8325921731742727\n",
      "\n",
      "TRAIN STD 1200.3486987386668\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 3229554.14300194, RMSE: 1797.096030545374, MAE : 342.6888225949348\n",
      "\n",
      "R2 TEST -0.0809716558525011\n",
      "\n",
      "TEST STD 941.3534568487511\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "[0.01400955 0.02006761 0.01216134 0.02026677 0.03044795 0.00719963\n",
      " 0.02421035 0.00604031 0.00801687 0.01998617 0.03254358 0.01089761\n",
      " 0.21831706 0.01505818 0.00268136 0.02750279 0.02536306 0.02493319\n",
      " 0.04433854 0.00244055 0.0039757  0.01180708 0.0240998  0.00236501\n",
      " 0.00044349 0.30172123 0.01672885 0.00108655 0.00899434 0.01022315\n",
      " 0.01796565 0.03410668]\n",
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 293650.0005973578, RMSE: 541.894824294676, MAE : 87.03722736497548\n",
      "\n",
      "R2 TRAIN 0.8325921731742727\n",
      "\n",
      "TRAIN STD 1200.3486987386668\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 3229554.14300194, RMSE: 1797.096030545374, MAE : 342.6888225949348\n",
      "\n",
      "R2 TEST -0.0809716558525011\n",
      "\n",
      "TEST STD 941.3534568487511\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "[0.01400955 0.02006761 0.01216134 0.02026677 0.03044795 0.00719963\n",
      " 0.02421035 0.00604031 0.00801687 0.01998617 0.03254358 0.01089761\n",
      " 0.21831706 0.01505818 0.00268136 0.02750279 0.02536306 0.02493319\n",
      " 0.04433854 0.00244055 0.0039757  0.01180708 0.0240998  0.00236501\n",
      " 0.00044349 0.30172123 0.01672885 0.00108655 0.00899434 0.01022315\n",
      " 0.01796565 0.03410668]\n",
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 293650.0005973578, RMSE: 541.894824294676, MAE : 87.03722736497548\n",
      "\n",
      "R2 TRAIN 0.8325921731742727\n",
      "\n",
      "TRAIN STD 1200.3486987386668\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 3229554.14300194, RMSE: 1797.096030545374, MAE : 342.6888225949348\n",
      "\n",
      "R2 TEST -0.0809716558525011\n",
      "\n",
      "TEST STD 941.3534568487511\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "[0.01400955 0.02006761 0.01216134 0.02026677 0.03044795 0.00719963\n",
      " 0.02421035 0.00604031 0.00801687 0.01998617 0.03254358 0.01089761\n",
      " 0.21831706 0.01505818 0.00268136 0.02750279 0.02536306 0.02493319\n",
      " 0.04433854 0.00244055 0.0039757  0.01180708 0.0240998  0.00236501\n",
      " 0.00044349 0.30172123 0.01672885 0.00108655 0.00899434 0.01022315\n",
      " 0.01796565 0.03410668]\n"
     ]
    }
   ],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [no_pollution_df.Total , no_pollution_df.Israelis_Count , no_pollution_df.Tourists_Count]\n",
    "\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "\n",
    "    X = no_pollution_df.drop(['Total' , 'Israelis_Count', 'Tourists_Count' ,'Date', 'Site_Name',] , axis=1)\n",
    "    \n",
    "    y = ys[index]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "    train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "    x_train_scaler = MinMaxScaler()\n",
    "    x_test_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "\n",
    "    y_train_scaled = np.log(y_train + 0.01)\n",
    "    y_test_scaled = np.log(y_test + 0.01)\n",
    "    dtree = DecisionTreeRegressor(random_state = 987456) \n",
    "\n",
    "    dtree.fit(X_train_scaled,y_train_scaled)\n",
    "    \n",
    "    #predict the training data\n",
    "    y_train_pred_scaled = dtree.predict(X_train_scaled)\n",
    "    #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "    y_train_pred = np.exp(y_train_pred_scaled)\n",
    "    #create a pandas series of the results\n",
    "    y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "    \n",
    "    #Add the results to the DF\n",
    "    train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "    #print(train_df.columns)\n",
    "    y_test_pred_scaled = dtree.predict(X_test_scaled)\n",
    "    y_test_pred = np.exp(y_test_pred_scaled)\n",
    "    #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "    #create a pandas series of the results\n",
    "    y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "    #Add the results to the DF\n",
    "    test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "    if train_df is None and test_df is None : continue\n",
    "    train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "    train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "    train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "    train_std = np.std(train_df.predicted_entries_train)\n",
    "    r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "    #print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "    print(\"------ TRAIN DATA ------\\n\")\n",
    "    print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "    print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "    print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "    try:\n",
    "        test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "    except: pass\n",
    "    try:\n",
    "        test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "    except: pass\n",
    "    try:\n",
    "        test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "    except: pass\n",
    "    try:\n",
    "        r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "    except: pass\n",
    "    \n",
    "    test_std = np.std(test_df.predicted_entries_test)\n",
    "    print(\"------ TEST DATA ------\\n\")\n",
    "    print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "    print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "    print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "    print(\"--------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    coeff = dtree.feature_importances_\n",
    "    print(coeff)\n",
    "    # features = list(X.columns)\n",
    "    # feat_zip = zip(features,coeff)\n",
    "    # feat_dict = dict(feat_zip)\n",
    "    # print(feat_dict)\n",
    "    #print(\"EQUATION : \" + equation)\n",
    "    #    new_row = {'Descripton':'general model ,Uni var feature selection, k=10, Degree: '+str(i),'Target':target,'Model_type':'POLY',\n",
    "    #    'Degree' : str(i),'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "    # 'MSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "    # 'AE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "    # 'EST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "\n",
    "    #poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "    #return train_df,test_df\n",
    "index += 1\n",
    "    #f.lose()\n",
    "#poly_rs.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_res = pd.read_excel(\"./poly.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [total_no_corr_no_outliers_week.Total , israelis_no_corr_no_outliers_week.Israelis_Count , tourists_no_corr_no_outliers_week.Tourists_Count]\n",
    "xs = [total_no_corr_no_outliers_week.drop(['Total' , 'Israelis_Count','Tourists_Count' ] , axis=1),\n",
    "israelis_no_corr_no_outliers_week.drop(['Total' , 'Israelis_Count' ] , axis=1),\n",
    "tourists_no_corr_no_outliers_week.drop(['Total' ,  'Tourists_Count' ] , axis=1)]\n",
    "\n",
    "for x in xs:\n",
    "    cols = x.columns\n",
    "    drop = [x for x in cols if x in cols_to_drop]\n",
    "    x.drop(drop,axis=1)\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    \n",
    "        print(target+\"\\n\")\n",
    "        #f = open(\"./\"+target+\"_general_UNI.txt\" , \"w\")\n",
    "        #to_drop = [x for x in targets if x != target]\n",
    "        #print(to_drop)\n",
    "        X = xs[index]\n",
    "        \n",
    "        y = ys[index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "\n",
    "\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        dtree = RandomForestRegressor(random_state = 987456 , n_estimators=5000,min_samples_leaf =50) \n",
    "\n",
    "        dtree.fit(X_train_scaled,y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = dtree.predict(X_train_scaled)\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        #print(train_df.columns)\n",
    "        y_test_pred_scaled = dtree.predict(X_test_scaled)\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        #print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        print(\"------ TRAIN DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        print(\"------ TEST DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "        \n",
    "        features = list(X.columns)\n",
    "        fig = go.Figure()\n",
    "        trace = go.Bar(\n",
    "            x = dtree.feature_importances_,\n",
    "            y = features,\n",
    "            orientation='h'\n",
    "        )\n",
    "        # fig.add_trace(trace)\n",
    "        # fig.write_image(\"./\"+target+\"_weekdays.png\",width = 2000,height = 1500)\n",
    "        \n",
    "        # coeff = dtree.feature_importances_\n",
    "        # print(coeff)\n",
    "\n",
    "        #print(\"EQUATION : \" + equation)\n",
    "        # new_row = {'Descripton':'weekdays, no outliers + corr, no pollutions','Target':target,'Model_type':'Random Forests','Feature_Selection':'corr >= 0.4',\n",
    "        # 'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "        # 'MSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "        # 'MAE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "        # 'TEST_STD':test_std ,'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df),'PARAMS':dtree.get_params}\n",
    "\n",
    "        # poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "        index += 1\n",
    "    #f.lose()\n",
    "# poly_res.to_excel(\"./poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per site trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Total' , 'Israelis_Count', 'Tourists_Count' ]\n",
    "ys = [df_no_outliers.Total , df_no_outliers.Israelis_Count , df_no_outliers.Tourists_Count]\n",
    "\n",
    "\n",
    "\n",
    "for site in sites:\n",
    "    index = 0\n",
    "    print(\"-----\"+site+\"-----\")\n",
    "    for target in targets:\n",
    "        print(target+\"\\n\")\n",
    "    #f = open(\"./\"+target+\"_general_UNI.txt\" , \"w\")\n",
    "    #to_drop = [x for x in targets if x != target]\n",
    "    #print(to_drop)\n",
    "\n",
    "    \n",
    "        X = df_no_outliers.drop(['Total' , 'Israelis_Count', 'Tourists_Count' ] , axis=1)\n",
    "        \n",
    "        y = ys[index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7254)\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        dtree = DecisionTreeRegressor(random_state = 987456) \n",
    "\n",
    "        dtree.fit(X_train_scaled,y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = dtree.predict(X_train_scaled)\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        #print(train_df.columns)\n",
    "        y_test_pred_scaled = dtree.predict(X_test_scaled)\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        #print(\"Degrees: \"+str(i)+\",Target: \"+target+\"\\n\")\n",
    "        print(\"------ TRAIN DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        print(\"------ TEST DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "        coeff = dtree.feature_importances_\n",
    "        #print(coeff)\n",
    "\n",
    "        #print(\"EQUATION : \" + equation)\n",
    "        #    new_row = {'Descripton':'general model ,Uni var feature selection, k=10, Degree: '+str(i),'Target':target,'Model_type':'POLY',\n",
    "        #    'Degree' : str(i),'MAE_Training':train_mae,'MSE_Training':train_mse,\n",
    "        # 'MSE_Training': train_rmse,'R2_Training': r2_train,\n",
    "        # 'AE_Test':test_mae,'MSE_Test':test_mse,'RMSE_Test': test_rmse,'R2_Test': r2_test,'TRAIN_STD':train_std,\n",
    "        # 'EST_STD':test_std , 'EQUATION' : str(equation),'TEST_SIZE' : len(test_df),'TRAIN_SIZE':len(train_df)}\n",
    "\n",
    "        #poly_res = poly_res.append(new_row,ignore_index=True)\n",
    "        #return train_df,test_df\n",
    "    index += 1\n",
    "    #f.lose()\n",
    "#poly_rs.to_excel(\"../POLI/poly.xlsx\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(tmp, drops):\n",
    "    cols = tmp.columns\n",
    "    res = [x for x in cols if x in drops]\n",
    "    return tmp.drop(res , axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site characteristics : trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sites = df.loc[df.is_water == 1]\n",
    "arch_sites = df.loc[df.is_archaeology == 1]\n",
    "water_sites = water_sites.dropna(axis = 0)\n",
    "arch_sites = arch_sites.dropna(axis = 0)\n",
    "#cleaning un relevant columns and stuff..\n",
    "cols = ['Tourists_Count','Israelis_Count','Total','Date','Model_number','Site_Name']\n",
    "\n",
    "water_sites_y = [water_sites.Total , water_sites.Israelis_Count,water_sites.Tourists_Count]\n",
    "water_sites_x = drop(water_sites,cols)\n",
    "\n",
    "\n",
    "arch_sites_y = [arch_sites.Total , arch_sites.Israelis_Count,arch_sites.Tourists_Count]\n",
    "arch_sites_x = drop(arch_sites,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "north = no_pollution_df.loc[(no_pollution_df['region_North'] == 1) & (no_pollution_df['is_weekend'] == 0)]\n",
    "central = no_pollution_df.loc[(no_pollution_df['region_Central'] == 1) & (no_pollution_df['is_weekend'] == 0)]\n",
    "south = no_pollution_df.loc[(no_pollution_df['region_South'] == 1) & (no_pollution_df['is_weekend'] == 0)]\n",
    "\n",
    "north.set_index('Date', drop=True , inplace=True)\n",
    "central.set_index('Date', drop=True , inplace=True)\n",
    "south.set_index('Date', drop=True , inplace=True)\n",
    "\n",
    "cols = ['Tourists_Count','Israelis_Count','Total','Model_number','Site_Name']\n",
    "\n",
    "north_y = [north.Total , north.Israelis_Count,north.Tourists_Count]\n",
    "north_x = drop(north,cols)\n",
    "\n",
    "\n",
    "central_y = [central.Total , central.Israelis_Count,central.Tourists_Count]\n",
    "central_x = drop(central,cols)\n",
    "\n",
    "south_y = [south.Total , south.Israelis_Count,south.Tourists_Count]\n",
    "south_x = drop(south,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total -  central \n",
      "\n",
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 530818.8496578876, RMSE: 728.573160127305, MAE : 399.8623653965607\n",
      "\n",
      "R2 TRAIN -0.3069648225619803\n",
      "\n",
      "TRAIN STD 362.682755280835\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 508481.5700541268, RMSE: 713.0789367623522, MAE : 390.57738559753847\n",
      "\n",
      "R2 TEST -0.28737451644347334\n",
      "\n",
      "TEST STD 366.58811862638566\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Israelis_Count -  south \n",
      "\n",
      "------ TRAIN DATA ------\n",
      "\n",
      "MSE : 275618.3032685353, RMSE: 524.9936221217695, MAE : 274.0824656156651\n",
      "\n",
      "R2 TRAIN -0.13096470085355127\n",
      "\n",
      "TRAIN STD 205.23920171152153\n",
      "\n",
      "------ TEST DATA ------\n",
      "\n",
      "MSE : 255358.78672080956, RMSE: 505.3303738355825, MAE : 267.25039754313605\n",
      "\n",
      "R2 TEST -0.12086208974878243\n",
      "\n",
      "TEST STD 204.64271905139842\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets = ['Total' , 'Israelis_Count']\n",
    "regions = ['central','south' ,'north']\n",
    "\n",
    "index = 0\n",
    "for target in targets:\n",
    "    \n",
    "        print(target+\" -  {} \\n\".format(regions[index]))\n",
    "\n",
    "        X = north_x\n",
    "        \n",
    "        y = north_y[index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "\n",
    "        train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)\n",
    "        test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)\n",
    "        x_train_scaler = MinMaxScaler()\n",
    "        x_test_scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = x_train_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = x_test_scaler.fit_transform(X_test)\n",
    "\n",
    "        y_train_scaled = np.log(y_train + 0.01)\n",
    "        y_test_scaled = np.log(y_test + 0.01)\n",
    "        dtree = RandomForestRegressor(random_state = 987456 , n_estimators=1000,min_samples_leaf =50) \n",
    "\n",
    "        dtree.fit(X_train_scaled,y_train_scaled)\n",
    "        \n",
    "        #predict the training data\n",
    "        y_train_pred_scaled = dtree.predict(X_train_scaled)\n",
    "        #y_train_pred = y_train_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_train_pred = np.exp(y_train_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_train_pred = round(pd.Series(y_train_pred, index=y_train.index, name='predicted_entries_train'),ndigits=2)\n",
    "        \n",
    "        #Add the results to the DF\n",
    "        train_df = pd.merge(left=train_df, right=y_train_pred , left_index=True, right_index=True)\n",
    "        #print(train_df.columns)\n",
    "        y_test_pred_scaled = dtree.predict(X_test_scaled)\n",
    "        y_test_pred = np.exp(y_test_pred_scaled)\n",
    "        #y_test_pred = y_test_scaler.inverse_transform(y_test_pred_scaled)\n",
    "        #create a pandas series of the results\n",
    "        y_test_pred = round(pd.Series(y_test_pred, index=y_test.index, name='predicted_entries_test'),ndigits=2)\n",
    "        #Add the results to the DF\n",
    "        test_df = pd.merge(left=test_df, right=y_test_pred , left_index=True, right_index=True)\n",
    "        if train_df is None and test_df is None : continue\n",
    "        train_mse = metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_rmse = np.sqrt(metrics.mean_squared_error(train_df[targets[index]], train_df.predicted_entries_train))\n",
    "        train_mae = metrics.mean_absolute_error(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "        train_std = np.std(train_df.predicted_entries_train)\n",
    "        r2_train = r2_score(train_df[targets[index]], train_df.predicted_entries_train)\n",
    "\n",
    "        print(\"------ TRAIN DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(train_mse)+\", RMSE: \"+str(train_rmse)+\", MAE : \"+str(train_mae)+\"\\n\")\n",
    "        print(\"R2 TRAIN \"+ str(r2_train)+\"\\n\")\n",
    "        print(\"TRAIN STD \"+str(train_std)+\"\\n\")\n",
    "        try:\n",
    "            test_mse = metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            test_rmse = np.sqrt(metrics.mean_squared_error(test_df[targets[index]], test_df.predicted_entries_test))\n",
    "        except: pass\n",
    "        try:\n",
    "            test_mae = metrics.mean_absolute_error(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        try:\n",
    "            r2_test = r2_score(test_df[targets[index]], test_df.predicted_entries_test)\n",
    "        except: pass\n",
    "        \n",
    "        test_std = np.std(test_df.predicted_entries_test)\n",
    "        print(\"------ TEST DATA ------\\n\")\n",
    "        print(\"MSE : \"+str(test_mse)+\", RMSE: \"+str(test_rmse)+\", MAE : \"+str(test_mae)+\"\\n\")\n",
    "        print(\"R2 TEST \"+ str(r2_test)+\"\\n\")\n",
    "        print(\"TEST STD \"+str(test_std)+\"\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Function\")\n",
    "import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function.plot_line(y_test_pred , y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a3f26f005c805b8796d2eac51f643ea47324abf4ab930a59c0414331385d455"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
